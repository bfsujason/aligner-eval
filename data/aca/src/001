THE FIRST PART OF MAN
CHAPTER I OF SENSE
CONCERNING the thoughts of man, I will consider them first singly, and afterwards in train or dependence upon one another.
Singly, they are every one a representation or appearance of some quality, or other accident of a body without us, which is commonly called an object.
Which object worketh on the eyes, ears, and other parts of man's body, and by diversity of working produceth diversity of appearances.
The original of them all is that which we call sense, (for there is no conception in a man's mind which hath not at first, totally or by parts, been begotten upon the organs of sense).
The rest are derived from that original.
To know the natural cause of sense is not very necessary to the business now in hand; and I have elsewhere written of the same at large.
Nevertheless, to fill each part of my present method, I will briefly deliver the same in this place.
The cause of sense is the external body, or object, which presseth the organ proper to each sense, either immediately, as in the taste and touch; or mediately, as in seeing, hearing, and smelling: which pressure, by the mediation of nerves and other strings and membranes of the body, continued inwards to the brain and heart, causeth there a resistance, or counter-pressure, or endeavour of the heart to deliver itself: which endeavour, because outward, seemeth to be some matter without.
And this seeming, or fancy, is that which men call sense; and consisteth, as to the eye, in a light, or colour figured; to the ear, in a sound; to the nostril, in an odour; to the tongue and palate, in a savour; and to the rest of the body, in heat, cold, hardness, softness, and such other qualities as we discern by feeling.
All which qualities called sensible are in the object that causeth them but so many several motions of the matter, by which it presseth our organs diversely.
Neither in us that are pressed are they anything else but diverse motions (for motion produceth nothing but motion).
But their appearance to us is fancy, the same waking that dreaming.
And as pressing, rubbing, or striking the eye makes us fancy a light, and pressing the ear produceth a din; so do the bodies also we see, or hear, produce the same by their strong, though unobserved action.
For if those colours and sounds were in the bodies or objects that cause them, they could not be severed from them, as by glasses and in echoes by reflection we see they are: where we know the thing we see is in one place; the appearance, in another.
And though at some certain distance the real and very object seem invested with the fancy it begets in us; yet still the object is one thing, the image or fancy is another.
So that sense in all cases is nothing else but original fancy caused (as I have said) by the pressure that is, by the motion of external things upon our eyes, ears, and other organs, thereunto ordained.
But the philosophy schools, through all the universities of Christendom, grounded upon certain texts of Aristotle, teach another doctrine; and say, for the cause of vision, that the thing seen sendeth forth on every side a visible species, (in English) a visible show, apparition, or aspect, or a being seen; the receiving whereof into the eye is seeing.
And for the cause of hearing, that the thing heard sendeth forth an audible species, that is, an audible aspect, or audible being seen; which, entering at the ear, maketh hearing.
Nay, for the cause of understanding also, they say the thing understood sendeth forth an intelligible species, that is, an intelligible being seen; which, coming into the understanding, makes us understand.
I say not this, as disapproving the use of universities: but because I am to speak hereafter of their office in a Commonwealth, I must let you see on all occasions by the way what things would be amended in them; amongst which the frequency of insignificant speech is one.
CHAPTER II OF IMAGINATION
That when a thing lies still, unless somewhat else stir it, it will lie still for ever, is a truth that no man doubts of.
But that when a thing is in motion, it will eternally be in motion, unless somewhat else stay it, though the reason be the same (namely, that nothing can change itself), is not so easily assented to.
For men measure, not only other men, but all other things, by themselves: and because they find themselves subject after motion to pain and lassitude, think everything else grows weary of motion, and seeks repose of its own accord; little considering whether it be not some other motion wherein that desire of rest they find in themselves consisteth.
From hence it is that the schools say, heavy bodies fall downwards out of an appetite to rest, and to conserve their nature in that place which is most proper for them; ascribing appetite, and knowledge of what is good for their conservation (which is more than man has), to things inanimate, absurdly.
When a body is once in motion, it moveth (unless something else hinder it) eternally; and whatsoever hindreth it, cannot in an instant, but in time, and by degrees, quite extinguish it: and as we see in the water, though the wind cease, the waves give not over rolling for a long time after; so also it happeneth in that motion which is made in the internal parts of a man, then, when he sees, dreams, etc.
For after the object is removed, or the eye shut, we still retain an image of the thing seen, though more obscure than when we see it.
And this is it the Latins call imagination, from the image made in seeing, and apply the same, though improperly, to all the other senses.
But the Greeks call it fancy, which signifies appearance, and is as proper to one sense as to another.
Imagination, therefore, is nothing but decaying sense; and is found in men and many other living creatures, as well sleeping as waking.
The decay of sense in men waking is not the decay of the motion made in sense, but an obscuring of it, in such manner as the light of the sun obscureth the light of the stars; which stars do no less exercise their virtue by which they are visible in the day than in the night.
But because amongst many strokes which our eyes, ears, and other organs receive from external bodies, the predominant only is sensible; therefore the light of the sun being predominant, we are not affected with the action of the stars.
And any object being removed from our eyes, though the impression it made in us remain, yet other objects more present succeeding, and working on us, the imagination of the past is obscured and made weak, as the voice of a man is in the noise of the day.
From whence it followeth that the longer the time is, after the sight or sense of any object, the weaker is the imagination.
For the continual change of man's body destroys in time the parts which in sense were moved: so that distance of time, and of place, hath one and the same effect in us.
For as at a great distance of place that which we look at appears dim, and without distinction of the smaller parts, and as voices grow weak and inarticulate: so also after great distance of time our imagination of the past is weak; and we lose, for example, of cities we have seen, many particular streets; and of actions, many particular circumstances.
This decaying sense, when we would express the thing itself (I mean fancy itself), we call imagination, as I said before.
But when we would express the decay, and signify that the sense is fading, old, and past, it is called memory.
So that imagination and memory are but one thing, which for diverse considerations hath diverse names.
Much memory, or memory of many things, is called experience.
Again, imagination being only of those things which have been formerly perceived by sense, either all at once, or by parts at several times; the former (which is the imagining the whole object, as it was presented to the sense) is simple imagination, as when one imagineth a man, or horse, which he hath seen before.
The other is compounded, when from the sight of a man at one time, and of a horse at another, we conceive in our mind a centaur.
So when a man compoundeth the image of his own person with the image of the actions of another man, as when a man imagines himself a Hercules or an Alexander (which happeneth often to them that are much taken with reading of romances), it is a compound imagination, and properly but a fiction of the mind.
There be also other imaginations that rise in men, though waking, from the great impression made in sense: as from gazing upon the sun, the impression leaves an image of the sun before our eyes a long time after; and from being long and vehemently attent upon geometrical figures, a man shall in the dark, though awake, have the images of lines and angles before his eyes; which kind of fancy hath no particular name, as being a thing that doth not commonly fall into men's discourse.
The imaginations of them that sleep are those we call dreams.
And these also (as all other imaginations) have been before, either totally or by parcels, in the sense.
And because in sense, the brain and nerves, which are the necessary organs of sense, are so benumbed in sleep as not easily to be moved by the action of external objects, there can happen in sleep no imagination, and therefore no dream, but what proceeds from the agitation of the inward parts of man's body; which inward parts, for the connexion they have with the brain and other organs, when they be distempered do keep the same in motion; whereby the imaginations there formerly made, appear as if a man were waking; saving that the organs of sense being now benumbed, so as there is no new object which can master and obscure them with a more vigorous impression, a dream must needs be more clear, in this silence of sense, than are our waking thoughts.
And hence it cometh to pass that it is a hard matter, and by many thought impossible, to distinguish exactly between sense and dreaming.
For my part, when I consider that in dreams I do not often nor constantly think of the same persons, places, objects, and actions that I do waking, nor remember so long a train of coherent thoughts dreaming as at other times; and because waking I often observe the absurdity of dreams, but never dream of the absurdities of my waking thoughts, I am well satisfied that, being awake, I know I dream not; though when I dream, I think myself awake.
And seeing dreams are caused by the distemper of some of the inward parts of the body, diverse distempers must needs cause different dreams.
And hence it is that lying cold breedeth dreams of fear, and raiseth the thought and image of some fearful object, the motion from the brain to the inner parts, and from the inner parts to the brain being reciprocal; and that as anger causeth heat in some parts of the body when we are awake, so when we sleep the overheating of the same parts causeth anger, and raiseth up in the brain the imagination of an enemy.
In the same manner, as natural kindness when we are awake causeth desire, and desire makes heat in certain other parts of the body; so also too much heat in those parts, while we sleep, raiseth in the brain an imagination of some kindness shown.
“We Are Not Equipped to Answer”
What of the future?
Over the course of a generation the Holocaust has moved from the margins to the center of American Jewish consciousness; from an event that rarely appeared in American public discourse to one that is omnipresent.
Will the Holocaust, one or two generations down the road, loom as large as it does now?
Will its centrality prove short-lived?
These aren't questions that anyone can answer with confidence, but looking at the idea of collective memory in general, and at what's happened so far with the memory of the Holocaust in particular, may offer a few clues.
Some collective memories are very longlived the Battle of Kosovo for Serbs, the expulsion of 1492 for Sephardic Jews.
The reason that these memories endured for centuries is that the conditions they symbolized also endured: foreign oppression, foreign exile.
Long-lived memories are most characteristic of stable, relatively unchanging societies.
When we speak of collective memory, we often forget that we’re employing a metaphoran organic metaphor that makes an analogy between the memory of an individual and that of a community.
The metaphor works best when we’re speaking of an organic (traditional, stable, homogeneous) community in which consciousness, like social reality, changes slowly.
When Maurice Halbwachs first advanced the idea of collective memory in the 1920s, the great French medievalist Marc Bloch, who was suspicious of organic metaphors for society, nevertheless thought it might be usefully applied to such things as a peasant grandparent, grandchild on knee, passing on rural traditions.
A very organic image.
How appropriate the metaphor is for the very inorganic societies of the late twentieth century (fragmented rather than homogeneous, rapidly changing rather than stable, the principal modes of communication electronic rather than face to face) seems to me questionable.
The life expectancy of memories in today’s society appears greatly diminished.
With the circumstances of our lives changing as rapidly as they do, it is the rare memory that can connect with an unchanging condition.
Nowhere is this more true than in the United States, that notoriously most "now” and amnesiac of nations.
Years ago, the Fourth of July was a day when the community gathered to hear patriotic speeches.
When was the last time you went to a patriotic speech on the Fourth of July?
When was the last time you even thought about the Declaration of Independence, which it commemorates, on that day?
“The world will little note, nor long remember what we say here, but it can never forget what they did here.”
But of course Lincoln got it backward.
His words at Gettysburg still (perhaps, sometimes) resonate; not one American in a hundred has the faintest notion of the significance of the Battle of Gettysburg, or who won it.
November 11 used to be Armistice Day, on which we paid homage to a memory; now it’s Veterans Day, on which we pay homage to an interest group.
Some of my colleagues continue to be shocked that incoming freshmen know nothing of such “ancient history” as the Vietnam War and Watergate.
My colleagues acknowledge grudgingly, because it makes them feel old that these things happened before the students were born, but protest that the kids’ parents lived through them; “they must have told them about it”!
Maybe the parents did, but the kids didn’t (as kids don’t) listen.
We’re a long way from traditional notions of the transmission of collective memories, let alone the sort that endure for centuries.
Whether or not we want to call it memory, consciousness of the Holocaust has grown enormously over the last generation most notably among American Jews, more diffusely among all Americans.
What does the process by which that growth took place suggest about “the future of the Holocaust”?
Much of the original impetus behind the Jewish drive for centering the Holocaust is, if not spent, at least declining in power.
Those who once complained, with justification, that the Holocaust had been neglected, and that it was necessary to counteract that neglect, have surely succeeded in their task, and the complaint no longer has any force.
As we’ve seen, Holocaust awareness was promoted to mobilize support for a beleaguered Israel, pictured as being in a kind of pre-Holocaust danger.
Such arguments are rarely heard anymore and would be unlikely to attract many adherents if they were heard.
Though the chances for a mutually satisfactory accommodation between Israelis and Palestinians remain in doubt, it’s hard to imagine the circumstances in which Israel’s problems will summon up images of the Holocaust.
In the early seventies there was a good deal of talk about a rampant “new anti-Semitism” in America, of the need to remind both Jews and gentiles of the Holocaust in order to combat it.
I’ve argued that at the time claims about a new anti-Semitism were nonsense; certainly they’re nonsense today.
While anti-Semites will probably always be with us, their influence, insignificant twenty-five years ago, is even more insignificant now.
A number of those who took the lead in promoting Holocaust awareness were moved by the belief that it called for a reorientation of Jewish religious belief and practice.
This view has failed to win much support, and its power seems to be waning.
Rabbi Irving Greenberg was perhaps the single most influential figure in centering the Holocaust in Jewish and general American consciousness, through the private commemorative activities he sponsored and through his service as director of President Carter’s Commission on the Holocaust.
For Greenberg, as we’ve seen, the Holocaust was a “revelational event” on a par with receiving the Torah at Mount Sinai.
As Jews ritually eat matzo to commemorate the Exodus, he urged them to ritually eat rotten potato peelings to commemorate the Holocaust.
Greenberg argued that just as the destruction of the Temple brought forth the new institution of the synagogue, so the destruction of European Jewry was bringing forth another new religious institution, the Holocaust museum.
But most of the Jewish religious establishment has resisted the efforts of Greenberg, Emil Fackenheim, and others who sought to win for the Holocaust a major place in formal Jewish liturgy and theology.
For Ismar Schorsch, head of the Jewish Theological Seminary (Conservative), the Holocaust was “a theological ‘black hole’ so dense that it fails to emit even a single ray of light.
A collapsed star can never serve as a source of illumination.”
Representatives of other segments of American Judaism have been equally opposed to seeing the Holocaust centered in Jewish religious belief, and nowadays there are few pressing this cause.
In a broad sense the Holocaust remains sacred in American “folk Judaism”, especially among the less observant, and this is likely to continue in the short term.
Whether this will endure without formal religious institutionalization is hard to say.
The argument for raising Holocaust consciousness that has been advanced with the greatest urgency is, by any sober evaluation, the most absurd: the alleged necessity of responding to the tiny band of cranks, kooks, and misfits who deny that the Holocaust took place.
Concern about the “growing influence” of this corporal’s guard was widespread for a time, but now seems to be abating.
The “star” of American Holocaust denial is Arthur Butz, an associate professor of electrical engineering at Northwestern University, who in 1976 arranged the private publication of The Hoax of the Twentieth Century: The Case Against the Presumed Extermination of European Jewry.
A few years later the Institute for Historical Review was established in California; its principal activity was publishing the Journal of Historical Review, a slender quarterly devoted to exposing “the myth of the six million”.
The institute rented the mailing list of the unsuspecting Organization of American Historians, and sent sample copies of its journal to the organization’s twelve thousand members, garnering some publicity from the resulting reaction.
More publicity came when its offer of a $50,000 reward for anyone who could prove any Jews were gassed at Auschwitz was taken up by Mel Mermelstein, a survivor of that camp; after bringing a lawsuit, he collected.
The deniers’ most successful publicity coup was their clever idea of sending college newspapers advertisements calling for “open debate” on the Holocaust.
A series of fusses were occasioned by several undergraduate editors’ notion that rejecting the ads raised “First Amendment issues”.
This kept the pot boiling, though in no case known to me was the Holocaust itself ever debated on campuses; rather, it was a question of whether boycott or exposure was the best strategy for dealing with these screwballs.
So far as one can tell, it was only fellow screwballs that they ever attracted: John Hinckley, who shot President Reagan, was a denier; so was Eric Rudolph, at this writing wanted for the murder of a guard at an abortion clinic; so was the crazed chess genius Bobby Fischer.
The activities of these fruitcakes were irritating, indeed infuriating especially, though not exclusively, to survivors.
Some insisted that the deniers should be taken more seriously, but since there was no evidence that they’d had the slightest influence, it was hard to say why one should do so.
Then seemingly powerful evidence of the deniers’ influence was supplied by a public opinion poll conducted for the American Jewish Committee by the Roper Organization.
Its results were announced on the eve of the opening of the Washington Holocaust Museum in 1993.
Twenty-two percent of the public, according to the poll, doubted that the Holocaust had really occurred.
“What have we done?
Twenty-two percent... oh, my God,” said Elie Wieselan understandable reaction to this astonishing statistic, and one echoed in the press across the country.
Coming on the eve of the museum opening, the poll finding was cited in newspaper editorials and other commentaries as the decisive argument for its indispensability, an unanswerable response to those who had doubted that such a museum belonged in the United States.
Deborah Lipstadt’s Denying the Holocaust: The Growing Assault on Truth and Memory arrived in bookstores at the same time as the poll results were announced.
It was the rare review of Lipstadt’s book that didn’t cite the poll as proof that the deniers had indeed had the success Lipstadt attributed to them.
For the next several months the growing menace of the deniers and the evidence of their astonishing influence offered by the Roper poll were repeatedly discussed.
The poll results were said to underscore the importance not only of the Washington museum but of Spielberg’s Schindler’s List in checking the denier menace.
Steven Spielberg himself said that he had feared, during the making of the movie, that if he made any mistakes, it would help the deniers.
All of this coincided with the height of the onslaught against American universities as hotbeds of nihilism and relativism.
Lipstadt, in her book, claimed that the willingness of campus editors to run deniers’ advertisements was evidence of the strength of postmodernism and deconstructionism in the universities.
This theme was eagerly picked up by conservative commentators still another lesson, though this time not of the Holocaust but of its denial.
(In fact, the student editors in question had not been immersing themselves in Foucault and Derrida, but instead reading, somewhat carelessly, those “dead white males” Thomas Jefferson and John Stuart Mill, and concluding that their principles required making a place in the marketplace of ideas for the deniers.)
It was a different sort of marketplace that “deconstructed” the Roper poll results.
Officials of the Gallup polling organization Roper’s main competitor were struck by the fact that the question that produced the astonishing 22 percent who doubted the Holocaust had, against all the rules of survey research, been framed in an extraordinarily confusing way: “Does it seem possible or does it seem impossible to you that the Nazi extermination of the Jews never happened.”
Seizing this opportunity to show up a rival’s incompetence, the Gallup people asked one sample group the original Roper question and presented another group with a straightforward version.
The version containing the double negative again produced dramatically high results.
1 Academic Freedom
IN THE SPRING OF 1977, Columbia University announced that it would offer Henry Kissinger a special chair in international relations amply endowed with funds for staff support and research.
In the weeks that followed, many students and professors vigorously protested the appointment, citing Dr. Kissinger's involvement in the bombing of Hanoi, the invasion of Cambodia, and the prolongation of the Vietnam War.
Eventually, the controversy died when Kissinger declared that he was not prepared to accept Columbia's offer.
But the episode left a number of issues unresolved.
Dr. Kissinger's attackers insisted that they were not violating his academic freedom by objecting to actions that he took as Secretary of State.
And yet, was Columbia at liberty to reject Dr. Kissinger's appointment on the basis of policies he had earlier espoused, simply because he advocated them not as a scholar, but as a public servant?
Should the administration have agreed to pass a moral judgment on those policies on the ground that they were no mere opinions but views translated into decisions causing death and destruction to thousands of people?
In considering a faculty appointment, should the university evaluate a candidate's behavior in public office if the actions involved matters, such as the bombing of Hanoi, that bore no clear relation to his field of scholarly competence?
These questions remind us that disputes over academic freedom have not disappeared.
In fact, they have grown more complicated in an age when faculty members have become involved with important controversies in the "real world."
Such problems deserve thoughtful consideration, since they bear directly on the way a university functions and the values that underlie its intellectual activities.
The Case for Academic Freedom
Freedom of expression, in all its forms, can be justified on two fundamental grounds.
For the individual, the right to speak and write as one chooses is a form of liberty that contributes in important ways to a rich and stimulating life.
To be deprived of such liberty is to lose the chance to participate fully in an intellectual exchange that helps to develop one's values, to make one's meaning of the world, to exercise those qualities of mind and imagination that are most distinctively human.
Beyond its significance to the individual, freedom of speech has traditionally been regarded in this country as important to the welfare of society.
Throughout history, much progress has occurred through growth in our understanding of ourselves, our institutions, and the environment in which we live.
But experience teaches us that major discoveries and advances in knowledge are often highly unsettling and distasteful to the existing order.
Only rarely do individuals have the intelligence and imagination to conceive such ideas and the courage to express them openly.
If we wish to stimulate progress, we cannot afford to inhibit such persons by imposing orthodoxies, censorship, and other artificial barriers to creative thought.
These reasons provide the intellectual foundation for the guarantees set forth in the First Amendment of our Constitution.
Even so, we must acknowledge that our commitment to free speech is more a matter of faith than a product of logic or empirical demonstration.
It is always possible that the exercise of this liberty will produce mistakes and misperceptions that will mislead the public and actually result in harmful policies.
In contrast to many other countries, however, ours has elected to guard against these dangers not by censorship, but by encouraging open discussion in which ideas can be subjected to criticism and errors can be corrected through continuing argument and debate.
Universities should be unreserved in supporting these principles, since freedom of expression is critical to their central mission.
This point deserves a word of elaboration, for now that universities have become more involved in the society and more important to its development, they have acquired different constituencies that view their purpose in somewhat different ways.
To parents and students, universities are chiefly places where young people can obtain an education and spend several pleasant, intellectually stimulating years.
To the learned callings, universities are the locus of leading professional schools that select able students and train them to serve as competent practitioners.
To the government, universities are vehicles to help achieve social goals, such as equal opportunity for minorities, as well as important sources of sophisticated knowledge needed for defense, foreign policy, medicine, and technological development.
To corporate and foundation executives, along with public officials, universities are also valuable repositories of expertise from which to gain advice in addressing complicated questions.
Though all of these perceptions are accurate, we could probably obtain most of the services mentioned without having to create a university.
For example, independent colleges could provide an excellent undergraduate education, as many of them do already.
Separate institutions could be formed to offer professional training adequate to meet our practical needs.
And consulting organizations could supply the specialized advice and analysis that public agencies and other institutions so often seek.
While the various functions of the university could be reorganized and redistributed in this fashion, something important would be lost.
Neither colleges, nor consulting organizations, nor professional training schools can satisfy society's need for new knowledge and discovery.
True, one could look to some sort of research institute to perform this function.
But even this alternative would not wholly replace what universities can supply.
It is the special function of the university to combine education with research, and knowledgeable observers believe that this combination has distinct advantages both for teaching and for science and scholarship.
Experience suggests that graduate students learn best from working directly with able and established professors actively engaged in their own research, while the latter benefit in turn from the stimulation they derive from inquiring young associates.
Without the marriage of teaching and research that universities uniquely provide, the conduct of scholarly inquiry and scientific investigation, as well as the progress of graduate training, would be unlikely to continue at the level of quality achieved over the past two generations.
In a society heavily dependent on advanced education and highly specialized knowledge, such a decline could be seriously detrimental to the public welfare.
If this unique combination of education and discovery is the chief contribution of the university, how can its progress be secured?
Apart from the libraries, laboratories, and other facilities important to research, two ingredients are especially important.
The first of these is the ability to recruit the ablest and most creative people that can be attracted into academic life.
The second critical element is an environment of freedom in which professors can do their work without constraints or external direction.
Highly intelligent, imaginative people tend to resist orders from above and do not do their best work under such conditions.
Even less desirable than centralized direction is the imposition of restraints on the kinds of ideas and hypotheses that scholars can publicly entertain, for such restrictions stifle the spirit of venture some inquiry while blocking off entire fields of investigation that seem threatening to those who have strong interests in maintaining the status quo.
For these reasons, academic freedom is not merely a reflection of society's commitment to free speech; it is a safeguard essential to the aims of the university and to the welfare of those who work within it.
Teachers and scholars have a vital stake in continuing to enjoy the liberty to speak and write as they choose, because their lives are entirely devoted to developing and expounding ideas.
Universities in turn have a critical interest in preserving free expression, for without that freedom they will be hampered in appointing the most creative scientists and scholars and will suffer from forms of censorship that will jeopardize the search for knowledge and new discovery that represents their most distinctive contribution.
Opposition to Academic Freedom
Despite the force of these arguments, attacks upon academic freedom have continued over the years since the doctrine was proclaimed in 1915.
For several decades, the assaults came chiefly from conservative groups disturbed by theories and ideas that seemed to undermine prevailing orthodoxies.
More recently, however, the greatest challenges to free expression on the campus have come from students and faculty on the Left who have launched vigorous attacks on professors involved in the Vietnam War or in disputes over matters affecting race.
For example, at least one academic association has condemned scholars for conducting research of a "racist" nature.
Moreover, in a survey published in 1976, 11 percent of the professors who responded agreed that "academic research on the genetic bases of differences in intelligence should not be permitted, because it serves to sustain a fundamentally racist perspective," and 18 percent supported the proposition that such research "should be discouraged because it can easily serve to reinforce racial prejudices."
Regardless of their political coloration, all attacks on academic freedom rest upon a single rationale.
To opponents of free expression, ideas are powerful; they frequently affect public policy and are used to justify important decisions.
As a result, misguided theories and specious arguments can cause considerable harm.
Granted, one can always try to counter such views with opposing arguments.
Yet only in science can errors be disproved decisively, and even there fallacious theories may linger for decades before they are finally put to rest.
In matters of social policy, shoddy ideas and dubious theories can persist indefinitely and be exploited by powerful people to justify inhumane and exploitative policies.
Hence, those who advocate censorship will acknowledge the value of free speech but argue that there are times when particular theories and opinions threaten to produce such grievous harm that they must be suppressed.
In presenting this view, opponents of academic freedom overlook two glaring weaknesses in their position.
In the first place, history reveals that those who try to stamp out heresy often make egregious mistakes.
Since the time of Socrates, intellectuals have been penalized under circumstances that seem embarrassing in retrospect.
Professors have been dismissed or denied appointments for upholding the right to strike, for advocating racial integration, for refusing to testify before the House Un-American Activities Committee, for supporting the recognition of Communist China.
As noted a figure as Bertrand Russell was barred from teaching at the City College of New York for propounding "immoral and salacious doctrines" that seemed to condone extramarital sex.
Groups that seek to impose their orthodoxies today will presumably concede these errors, but will insist that their own opinions rest on firmer ground.
Yet those who attacked professors in the past must also have believed in the rightness of their cause.
Insofar as we can tell, their actions stemmed from a sincere belief that it would be harmful to students and to the public at large to employ teachers who expressed ideas and supported policies that seemed at the time to be dangerously misguided.
Despite these good intentions, the results were often sadly mistaken, and we should be honest enough to acknowledge that similar errors will occur if we begin to penalize professors for their moral, political, or economic beliefs.
The second fallacy in all attempts at censorship is the implicit assumption that only like-minded people will be able to decide which orthodoxies to impose.
Bankers and industrialists proceeded on this premise at the turn of the century when business values predominated in the society.
Radicals seem to have acted on the same assumption during the brief period in the late 1960s when their militant tactics threatened to push all opposition aside on campuses across the country.
But history suggests that no faction can wield decisive influence indefinitely and that groups with differing views will be ready to wage war against unpopular opinions whenever the opportunity arises.
It required no more than a decade to discover how grossly the radicals overestimated their power and how quickly they were followed by fundamentalist groups and conservative forces intent on pressing their own values on television programming, school textbooks, and other means of communication.
Ironically, the very professors who disdained academic freedom only ten years ago could soon find themselves clinging to the doctrine to protect themselves against the tide of conservative ideologies.
Behavioral scientists were first brought into the design process to shed light on the needs of future occupants.
Since psychologists had expert knowledge on the ways in which people responded to light, color, and proportion, sociologists about the nature of social organization in neighborhoods, and anthropologists on buildings as symbols and artifacts, it was hoped that they would be able to apply this knowledge to the design process.
This approach faltered for a number of reasons.
The findings collected in basic research could not be applied with any confidence.
Color perception was one of the oldest topics in experimental psychology, but if an experimental psychologist were asked to specify the colors that should be used in a bus station, the answer was likely to be vague and uncertain.
Nor was research on space perception, another traditional research topic, useful for understanding orientation in buildings and neighborhoods.
Sociologists had done considerable research on neighborhood networks, but their findings were usually too general to assist the city planner.
Needs analysis went through several stages in its evolution and is still changing.
At the outset, designers relied on needs they inferred or extrapolated from basic research studies and from theory.
More recent approaches have tended to dwell less upon inferred needs and more upon direct expression of people's wants, which are obtained through surveys or by the involvement of users in the design process.
The relationship between needs and wants is full of pitfalls for the unwary.
A need is something basic, fundamental, and enduring, as in the need for shelter, sustenance, and social contact, while a want is an expressed preference, and thus is likely to be specific, transient, and more easily changed, as in the desire for a particular brand of food or clothing style.
Human needs have traditionally been regarded as more acceptable topics of study than wants in the behavioral sciences.
The investigation of preferences is considered to be a second-rate applied task, more akin to marketing than to serious scholarship.
This dual standard has its roots in the platonic view that appearances are deceptive and ephemeral and that the important reality lies underneath.
However in some specific eases a firm distinction between needs and wants is difficult to maintain.
Some architects and urban planners looked to the behavioral sciences not for specific bits of information that could be applied on particular projects but for general theories of human behavior that could be generalized to many settings.
Some designers found what they wanted to know about perception in the writings of James J. Gibson and Rudolf Arnheim; about the desire for stimulation and variety in the work of D. O. Hebb and Daniel Berlyne; and about social life within neighborhoods from Herbert Gans and Louis Wirth.
This interest in general theories remains strong today and co-exists with the desire for place-specific information.
A mutual learning regarding what each can obtain from the other has occurred in the dialogue between designers and behavioral scientists.
Initially, architects complained that the social scientists' answers were too general, tentative, and often came too late to be useful.
Behavioral scientists criticized the architects' queries as being too specific, time-pressured, and deterministic in assuming a direct connection between building elements and behavioral patterns.
One of the most useful contributions that social scientists have made to design was to point out that design does not alone determine behavior.
Even when it strongly influences how people behave, the effects are not direct but are mediated through psycho-social factors; for example, different people will be affected differently by the same building, depending upon whether they are alone or in a group, happy or sad, or are visiting for the first time or are accustomed to the place.
Eventually those designers who wanted to collaborate with behavioral scientists learned how to ask questions that could be answered within the time frame, methods, and data base of the social sciences, and those behavioral scientists active in the collaboration learned to produce timely and relevant reports.
All this took time and patience on everyone’s part.
Effective collaboration between people in different fields, especially those as distinct in training and temperament as design and behavioral science, does not come easily.
Needs Assessment Survey
Another approach to needs analysis came not from basic research or from theory, but from marketing, and involved asking people what they wanted in the way of work or living space.
This blurs the distinction between needs and wants, since it assumes that people’s responses will bear some relationship to both.
Studies of expressed preference are of interest to the social scientist and useful to industry.
The value of such studies increases if they identify the particular properties inherent in the object on which the preference or rejection is based.
Technically it would have been more accurate to title this chapter Wants Analysis, but I have followed the conventional practice of discussing “needs”, since, from a practical standpoint, needs are more saleable than wants.
Managers are more likely to respond positively when they are told that employees need something than when they are told that employees want it.
The latter statement is likely to provoke the response, “Yes, we know they want it, but we can't a fiord to give it to them.
However, if our workers need it then that is another matter.”
In some ways this is a semantics game, more akin to diplomacy than science.
Economists speak of actual behavior in the marketplace as “revealed preferences,” meaning that the behavior of people in purchasing goods and services reveals their wants and preferences.
This approach is not valid in mast designed spaces that are not “purchased” by their occupants.
Nor can people’s attitudes towards the architecture or interior layout be discerned by how long they remain inside.
The failure of market mechanisms to reveal preferences and levels of occupant satisfaction with designed spaces created a niche for formal techniques for gauging occupant response.
Most designers claim to be meeting market demands to some degree, but the distinctive feature of UNA (user needs analysis) is that it is carried out in a systematic manner.
This requires the use of standardized techniques for collecting information (interviews, questionnaires, and so on) and some sampling of occupants and their activities.
Without formal procedures for selecting people to be interviewed and ways of asking questions, the technique does not qualify as a UNA.
It is not always safe to assume that input from potential occupants of a setting will be more relevant to the design process than input from those in similar facilities.
It may be that the potential occupants have never experienced a building of the same type before.
For example, asking teachers and students in a traditional school building what their space and environmental needs would be in an open-plan school building with modular pods housing three grades each may turn up little useful information, unless potential occupants are given some education or experience with open-plan school buildings.
Consulting with potential occupants is always desirable when it is possible, both because of the possibility of securing useful information about the residents and their space needs，and because it will satisfy people’s desire to be consulted about changes that affect their lives.
An assumption in a needs assessment survey is that potential occupants know what they want and can communicate this to an interviewer.
In cases where people are turned off to their physical surroundings and alienated from the decision-making process, this assumption is not valid.
Most people asked about civic improvements will respond hesitantly and tentatively.
They have not given much thought to these matters.
There was no reason for them to be concerned since their opinions had not mattered.
Even if they have an idea about what they want, they may not have the concepts to convey this to a designer.
This calls for some patience on the part of the interviewer in asking the right questions and in waiting for answers to come, just as listening to the speaker of another language requires special patience.
Drawings and models can make design alternatives more tangible to the prospective occupants.
People can be shown various options and asked to choose among them.
The occupant can move around parts of a scale model or even take part in a slide simulation study showing how a building facade will appear in different exterior colors.
The needs survey will be more meaningful when the designer has spent some time among the occupants and knows how they think and express themselves.
It would be extremely difficult to conduct a needs analysis in a specialized building without knowing the technical vocabulary of the residents.
Experiencing how people use space as a preliminary to design is exemplified in the work of landscape architect Lawrence Halprin.
Before the drawings are put on paper, Halprin and his staff go out into settings like those for which a design is to be conceived and spend time there in order to develop a choreography of people-place relationships in which movement patterns arc paramount.
Halprin has used these choreographs to design such successful places as San Francisco’s Ghiradelli Square and the Forecourt and Lovejoy Fountains in Portland, Oregon.
These works illustrate that a design that takes people's activities and interests into account can be exciting, attractive, and in the case of commercial buildings, economically successful.
What Halprin did not do was create sculptural forms in his studio and impose them on a location.
It has been argued that architects should live in the types of buildings that they are designing.
The Urban Development Corporation in New York City has required this of their architects for public housing projects for some time.
This is not a formal needs analysis, but it is one way of ensuring that the architect comes into direct contact with future occupants.
Living among the occupants works better in some places than in others.
In preparing an architectural plan for a university, architect Neal Deasy lived for a week in a dormitory and took all his meals in the dining hall.
Dressed in a conservative tweed suit and vest, he felt he was “as conspicuous as a piebald aardvark”.
The approach proved more successful in the outdoor areas of the campus.
Deasy started his observations at 7:30 a.m. and concluded at 10 p.m.
He followed a set course through the campus and noted what was taking place, who was involved, and what they were doing.
Photographs were taken at key locations and were printed on inexpensive proof sheets.
Having the photographs independently analyzed served as a check on the observer.
The pictures were useful in transmitting the architects recommendations to his client.
Photographs are often more persuasive than words for conveying the realities of human behavior, especially when the behavior is at odds with conventional expectations about how an area is being used.
Kathryn Anthony has been documenting the costs and benefits of needs analysis in several construction projects undertaken by the architectural office of Patrick Sullivan Associates in San Luis Obispo, California.
She has been able to show how the needs analysis lowered project costs.
In the case of two jails, the research showed that a lower level of security hardware and construction were feasible, and in the construction of a city hall, how similar types of work stations could be consolidated.
On the city hall project, which was scaled down 15 percent because of the efficiency of the open-office modular work stations recommended by the researchers, the client was billed for under 1,300 consultant hours.
A criticism of needs analysis is that it will awaken false hopes if people assume that they will always get what they request.
This expectation can be avoided, or at least minimized, by a clear statement in the survey that it may not be possible to fulfill everyone's wishes and by clearly stating at the outset the scope, resources, and limitations of the proposed project.
On the other hand, it is necessary that people believe that their suggestions will receive a fair hearing.
Otherwise their replies are not likely to be very useful for the UNA.
Chapter 3 Narrating the anti-conquest
At times indeed the Company officials allowed the main slave depot in Cape Town to be used as a son of brothel.
(Philip Curtin et al., African History (1978))
It is a relief to turn from these scenes of contention and disorder to notice the efforts that were made at this time [1793] by several colonists to improve the domestic animals of the country.
(George M.Theal, A History of Southern Africa (1907))
The previous chapter introduced the eighteenth-century systematizing of nature as a European knowledge-building project that created a new kind of Eurocentered planetary consciousness.
Blanketing the surface of the globe, it specified plants and animals in visual terms as discrete entities, subsuming and reassembling them in a finite, totalizing order of European making.
Perhaps one should be more specific about the terms: “European” in this instance refers above all to a network of literate Northern Europeans, mainly men from the lower levels of the aristocracy and the middle and upper levels of the bourgeoisie.
“Nature” meant above all regions and ecosystems which were not dominated by “Europeans,” while including many regions of the geographical entity known as Europe.
The project of natural history determined many sorts of social and signifying practices, of which travel and travel writing were among the most vital.
For the purposes of this book, what is of chief interest is the mutual engagement between natural history and European economic and political expansionism.
As I suggested above, natural history asserted an urban, lettered, male authority over the whole of the planet; it elaborated a rationalizing, extractive, dissociative understanding which overlaid functional, experiential relations among people, plants, and animals.
In these respects, it figures a certain kind of global hegemony, notably one based on possession of land and resources rather than control over routes.
At the same time, in and of itself, the system of nature as a descriptive paradigm was an utterly benign and abstract appropriation of the planet.
Claiming no transformative potential whatsoever, it differed sharply from overtly imperial articulations of conquest, conversion, territorial appropriation, and enslavement.
The system created, as I suggested above, a Utopian, innocent vision of European global authority, which I refer to as an anti-conquest.
The term is intended to emphasize the relational meaning of natural history, the extent to which it became meaningful specifically in contrast with an earlier imperial, and prebourgeois, European expansionist presence.
This chapter undertakes to illustrate more concretely the impact of natural history and global science on travel writing.
Through a set of examples, I aim to suggest how natural history provided means for narrating inland travel and exploration aimed not at the discovery of trade routes, but at territorial surveillance, appropriation of resources, and administrative control.
This discussion is intended to be read in conjunction with two subsequent chapters, which take up sentimental travel writing, the other main form of anti-conquest in this period.
In travel literature, I argue, science and sentiment code the imperial frontier in the two eternally clashing and complementary languages of bourgeois subjectivity.
In what follows I examine a sequence of four North European travel books on southern Africa written across the eighteenth century and spanning what I have been calling the Linnaean watershed: Peter Kolb’s The Present State of the Cape of Good Hope (Germany, 1719); Anders Sparrman’s Voyage to the Cape of Good Hope (Sweden, 1775); William Paterson’s Narrative of Four Voyages in the Land of the Hottentots and the Kaffirs (Britain, 1789), and John Barrow’s Travels into the Interior of Southern Africa (Britain, 1801).
My aim here is not to survey the extensive literature of travel on southern Africa in this period; rather, I have selected four texts that particularly illustrate the discursive impact of natural history and the new planetary consciousness.
(A contrasting instance of southern African travel writing is taken up in the next chapter.)
My observations coincide at a number of points with those of J.M. Coetzee in his 1988 study White Writing: On the Culture of Letters in South Africa.
The opening chapters of this valuable book focus heavily on eighteenth- and nineteenth-century travel writing on South Africa, including the writers discussed here.
Coetzee goes on to examine how European problematics of representation carry forward into nineteenth- and twentieth-century literature in South Africa, as I have tried to do for Spanish America in chapter 7 below.
The literature on the Cape of Good Hope is a particularly fruitful one for studying the discursive shifts in travel writing, for the Cape was one place where scientific travel, the momentum for inland expansion, and the shifting relations of contact these engendered, played themselves out conspicuously and dramatically.
The “great age” of scientific travel is usually associated with the South Sea expeditions of Cook, Bougainville, and others, first organized around the transit of Venus in 1768.
These maritime expeditions indeed inaugurated the era of scientific travel, and scientific travel writing.
But at the same time, they marked an end: the last great navigational phase of European exploration.
Cook discovered and mapped the shores of the last uncharted continent, Australia.
In a way, he set the stage for the new phase of inland exploration.
The Cape of Good Hope was one of the few places in Africa where Northern Europeans had access to the continental interior.
It was a magnet both for settlers and for explorers eager to make their mark.
It was a place where interior colonization broke into open conflict with sea-oriented mercantilism, where competition among European nations was played out as warfare.
In the early decades of the nineteenth century, as inland expansion proceeded, southern Africa was also to become a canonical test site for the civilizing mission in the labors of the London Missionary Society and their unmanageable star, David Livingstone.
Established in 1652 by the Dutch East India Company as a supply port for commercial ships, the Cape Colony proved a vital stopping-off point for European travelers of all kinds.
Fresh meat was available from the indigenous Khoikhoi (“Hottentot”) population, while the Company grew fresh vegetables to combat scurvy, provided leisure, cared for sick sailors, supplied ships with healthy crewmembers, and so on.
Vulnerable to attack and dependent on the indigenous cattle-herding population for fresh meat, the Company initially took pains to minimize its encroachment on the region and its exploitation of indigenous labor.
A proposal to attempt enslaving the Khoikhoi in 1654 was rejected; slaves were procured initially from West Africa, then from Malay and Ceylon.
Nevertheless, frontier conflict was immediate and constant (the first cross-racial murder recorded was in 1653), and intensified greatly in the 1670s as interior colonization by Europeans expanded as well.
Within a few years of founding the Cape Colony, the Dutch East India Company reluctantly agreed to grant a portion of the inhabitants the status of “free burghers,” or independent farmers, and to allow them to wrest grazing and farming lands from the cattle-rearing indigenous peoples.
This population of independent colonists grew slowly, largely out of the ranks of Company workers, stranded sailors, and African or Euroafrican women.
(Till 1685 there were no racial prohibitions on marriage; in 1685 marriages were outlawed between Europeans and Africans, but not between Europeans and persons of mixed blood.)
The settlers’ numbers were substantially increased in 1689 by 150 Huguenot dissenters from Holland, who brought with them the Dutch Reformed Church.
In 1699 the free burgher (Boer) population, the ancestors of today’s Afrikaners, numbered just over 1,000 men, women, and children, owning an unspecified number of slaves.
A century later, they were 17,000 plus 26,000 slaves.
Today they number two million.
The outlines of today’s Afrikaner agro-pastoral society — and today’s South African race war — were thus already in place by 1700.
The prison at Robben Island, where Nelson Mandela and the founders of the African National Congress were held during the 1960s, was established in 1657 for Hottentots “who assaulted or robbed a burgher.”
Largely outside the control of Company administration, and often at odds with the Company’s interests, free burgher society developed along its own expansive lines, pressing its way inland, usually in conflict and occasionally in alliance with resident Khoikhoi chiefdoms.
By force of horses (which indigenous Africans were forbidden by law to own), firearms (which European settlers were required by law to own), and strategic alliances among rival groups, the Europeans gradually overcame indigenous control and broke up local socioeconomic structures.
Smallpox epidemics in 1713, 1755, and 1767 weakened the indigenous position.
Gradually more and more Khoikhoi were forced into the role of subsistence laborers, herding the Boers’ cattle rather than their own.
By 1778 the new governor Von Plattenburg reported finding no autonomous Khoikhoi communities in the Cape Colony.
Which is not, of course, to say that indigenous society and indigenous resistance to colonization ended here; both continued in forms which I will discuss further below.
From the beginning of their presence, Europeans in the Cape periodically mounted expeditions to explore the interior.
One early object of interest, typical of the seventeenth century, was a mythical, gold-producing empire known as Monomatapa, akin to the El Dorado so long sought in the Americas.
These early expeditions were not seen as having produced any discoveries of value; nor, in the era of navigational narrative, did they result in travel books.
Not till the early eighteenth century did a European literature on southern Africa begin in earnest, one of the first major contributions being Peter Kolb’s The Present State of the Cape of Good Hope.
PETER KOLB AND THE REVINDICATION OF THE HOTTENTOTS
Published in German in 1719, Kolb’s book was translated into Dutch (1721), English (1731), and French (1741), and remained one of the main print sources on southern Africa through the first half of the century.
Trained as a mathematician, Kolb was sent to the Cape in 1706 by a Prussian patron to carry out astronomical and meteorological research.
Though his mission was scientific, Kolb’s account,like that of La Condamine in South America, was not.
His book, like La Condamine’s, contrasts in a number of respects with writings on the other side of the Linnaean watershed.
It is devoted mainly, as the title page puts it, to “A Particular ACCOUNT of the several NATIONS of the HOTTENTOTS: Their Religion, Government, Laws, Customs, Ceremonies, and Opinions; Their Art of War, Professions, Language, Genius, andamp;c together with A Short ACCOUNT of the DUTCH SETTLEMENT At the CAPE.”
Kolb’s account consists in the main of vivid ethnographic description of Khoikhoi society and lifeways in the traditional mode of manners and customs description.
While his account is based on what Kolb describes as years of contact with many different groups of Hottentots, this contact itself is not narrated, nor are Kolb’s travels in the interior.
Kolb was writing before narrative paradigms for interior travel and exploration emerged in the last decades of the century.
In 1719 navigational paradigms still prevailed: the only part of his experience Kolb does present as narrative is his six-month sea voyage to arrive at the Cape.
In keeping with the conventions of navigational narrative, the trip is told as a survival story complete with storms, sickness, brackish water, and threat of attack on the high seas.
As its title promises, Kolb’s account includes chapters on Khoikhoi forms of government, religion, ceremonies, domestic economy, cattle management, medicine, and so on.
It is easy to vouch for the vividness of the description, but less easy to speak for its accuracy.
Kolb declares he “made it a Rule not to be believe any Thing I did not see of which a Sight could be had,” but in the very next sentence confirms having seen “that Negroes are born White” and change color several days later!
Nevertheless, his account is undeniably the most substantive source on the indigenous people of the Cape in this period.
Here is a representative passage, to give something of the flavor of his writing:
Chapter 1 Cosmic rays
What are cosmic rays?
Cosmic ray particles hit the Earth's atmosphere at the rate of about 1000 per square meter per second.
They are ionized nuclei - about 90% protons, 9% alpha particles and the rest heavier nuclei - and they are distinguished by their high energies.
Most cosmic rays are relativistic, having energies comparable to or somewhat greater than their masses.
A very few of them have ultrarelativistic energies extending up to 1020 eV (about 20 joules), eleven orders of magnitude greater than the equivalent rest mass energy of a proton.
The fundamental question of cosmic ray physics is, "where do they come from?" and in particular, "how are they accelerated to such high energies?"
The answer to the question of the origin of cosmic rays is not yet fully known.
It is clear, however, that nearly all of them come from outside the solar system, but from within the galaxy.
The relatively few particles of solar origin are characterized by temporal association with violent events on the sun and consequently by a rapid variability.
In contrast, the bulk of cosmic rays show an anticorrelation with solar activity, being more effectively excluded from the solar neighborhood during periods when the expanding, magnetized plasma from the sun - the solar wind - is most intense.
The very highest energy cosmic rays have gyroradii in typical galactic magnetic fields that are larger than the size of the galaxy.
These may be of extragalactic origin.
1.2 Objective of this book
The focus of this book is the interface between particle physics and cosmic rays.
Until the advent of accelerators, cosmic rays and their interactions were the main source of information about elementary particles.
Although the highest energy cosmic rays can still offer clues about particle physics above accelerator energies, this is no longer the dominant aspect of the field.
There are now, however, a number of important areas in which a knowledge of particle interactions is necessary to understand the astrophysical implications of cosmic ray data.
Examples include:
Production of secondary cosmic rays such as antiprotons by primary cosmic rays when they collide with atomic nuclei in the interstellar medium.
From the relative amounts of such secondaries we learn about how cosmic rays propagate through the interstellar medium and hence about the nature of the matter and fields that make up the medium.
Production of photons, neutrinos and other particles in collisions of cosmic rays with material near a site of cosmic ray acceleration.
Seeing point sources of such particles is a way of identifying specific sources of cosmic ray acceleration and studying how they work.
Penetration of cosmic rays underground and the detection of muons and neutrinos in large, deep detectors.
Such particles can be both signal (for example, neutrinos from the point sources just mentioned) and background (for example, for the search for proton decay or magnetic monopoles).
The relation between atmospheric cascades and the incoming cosmic rays that produce them.
The highest energy cosmic rays are so rare that they cannot be directly observed with small detectors above the atmosphere, but they must be studied indirectly by large air shower arrays exposed for long periods at the surface.
Then one has to infer the nature of the primary from its secondary cascade.
Searches for exotic particles and new interactions in the cosmic radiation.
These topics clearly have a great deal in common: The same equations that govern particle cascades in the atmosphere of the Earth also describe particle production by cosmic rays accelerated by a collapsed star which then collide in a surrounding supernova envelope or in the atmosphere of a nearby companion star.
The same cross sections that determine the neutrino-induced signal in an underground detector also determine how much energy is absorbed by a companion star due to interactions of neutrinos produced by cosmic rays accelerated in the system.
The purpose of this book is to discuss the relevant particle physics and illustrate the application to cosmic ray astrophysics with examples of current interest.
At the same time I will try to provide a sufficient overview of cosmic ray physics so the importance of these examples can be appreciated.
1.3 Types of cosmic ray experiment
The principal data about the cosmic rays themselves, from which one can hope to learn about their origin, are the relative abundances of the different nuclei (composition) and the distribution in energy (energy spectrum) of each component.
Comparison with the chemical composition of various astrophysical objects, such as the Sun, the interstellar medium, supernovae or neutron stars, can give clues about the site at which cosmic rays are injected into the acceleration process.
The energy spectra may be characteristic of certain acceleration mechanisms.
Figure 1.1 gives a global view of the total cosmic ray energy spectrum.
Because of the enormous range of fluxes, it is useful to plot the flux per logarthmic interval of energy (EdN/dE=dN/dlnE).
Even so the vertical axis is extremely compressed.
A moment’s thought about the range of rates in fig. 1.1 will convince you that several quite different kinds of detectors are necessary to study cosmic rays over this whole energy range.
In the interval around one GeV, about 10 particles per second cross a telescope consisting of two planes of area 100 cm2 separated by 20 cm (see fig. 1.2).
A small detector flown at the top of the atmosphere in a balloon or spacecraft is therefore sufficient to study details of cosmic ray composition in the GeV energy region down to the level of one part in a hundred thousand.
In contrast, the University of Chicago detector on Spacelab (fig. 1.3) is one of the largest ever flown, so it is sensitive to cosmic rays of much higher energy.
It was carried on board the Challenger spacecraft from July 29 to August 6, 1985, during which time it had an effective exposure of 94 hours.
It has an aperture of approximately 2 m2sr.
From fig. 1.1 we can therefore estimate that during this exposure nearly 100 000 particles with energies within a factor of two of 1 TeV passed through the detector.
The spectrum is so steep, however, that only about 50 particles above 100 TeV would have passed through the detector during the flight, and most of these would likely have had charge below the charge threshold of the detector, which was designed for heavy nuclei.
An earlier Soviet spacecraft experiment (Grigorov et al., 1970), as well as the Japanese-American balloon experiment (Burnett et al., 1983) also probed similar energies.
For the time being, because of the limited exposure available, 100 TeV is about the highest energy at which cosmic rays can be studied directly with detectors at the top of the atmosphere.
Problem: The instrument in fig. 1.2 consists of two circular detectors in coincidence, each with 100 cm2 area and separated by 20 cm.
The maximum angle that will trigger the detector depends on the point of intersection of the trajectory with the first detector as well as its direction.
Show that the acceptance of this detector is ≈ 22cm2 sr for downward-going particles.
To study cosmic rays with higher energies requires detectors with larger areas exposed for longer periods of time.
At present the only way to overcome the problem of low flux at high energy is to build a detector on the surface of the Earth.
Such detectors, called air shower arrays, can have areas measured in square kilometers and exposure times limited only by the patience of the experimenters and the generosity of the funding agencies.
Ground-based detectors cannot detect the primary cosmic rays directly, but only the remnants of the atmospheric cascades of particles initiated by the incident particle.
They therefore give only limited, indirect information about the nature of the primaries.
Despite the obvious difficulty of the subject, the ultra-high energies involved continue to stimulate interest in it.
A knowledge of particle physics is essential in order to interpret the cascades and infer something about the primaries.
We will return to this subject in the final section of the book.
For the remainder of this chapter we focus on the bulk of the cosmic rays at lower energies.
1.4 Composition
The relative abundances of cosmic rays are compared with abundances of elements in the solar system in fig. 1.4.
The symbols in fig. 1.4 have the following meanings: Solid circles: low energy data, 70 - 280 MeV/A; open circles: high energy data, 1000 - 2000 MeV/A. Solar system abundances are shown by open diamonds.
Both solar system and cosmic ray abundances show the odd even effect, with the more tightly bound, even Z nuclei being more abundant.
There are, however, two striking differences between the two compositions.
First, nuclei with Z'1 are much more abundant relative to protons in the cosmic rays than they are in solar system material.
This is not really understood, but it could have something to do with the fact that hydrogen is relatively hard to ionize for injection into the acceleration process, or it could reflect a genuine difference in composition at the source.
The second difference is well understood and is an important tool for understanding propagation and confinement of cosmic rays in the galaxy.
The two groups of elements Li, Be, B and Sc, Ti, V, Cr, Mn are many orders of magnitude more abundant in the cosmic radiation than in solar system material.
These elements are essentially absent as end products of stellar nucleosynthesis.
They are nevertheless present in the cosmic radiation as spallation products of the abundant nuclei of carbon and oxygen (Li, Be, B) and of iron (Sc, Ti, V, Cr, Mn).
They are produced by collisions of cosmic rays in the interstellar medium (ISM).
From a knowledge of the cross sections for spallation, one can learn something about the amount of matter traversed by cosmic rays between production and observation.
(Note the implication that secondaries such as photons, neutrinos and antiprotons should also be produced at a certain rate as cosmic rays propagate through the ISM.
We shall return to this subject in chapter 10.)
For the bulk of the cosmic rays the mean amount of matter traversed is of order X = 5 to 10 g/cm2.
The density . . . in the disk of the galaxy is of order one proton per cm3, so this thickness of material corresponds to a distance of . . ..
Since the cosmic rays may spend some time in the more diffuse galactic halo, this is a lower limit to the distance travelled.
In any case, l''d≈0.1 kpc, the half-thickness of the disk of the galaxy.
This implies that cosmic ray confinement is a diffusive process in which the particles rattle around for a long time before escaping into intergalactic space.
1.5 Energy spectra
The spectra for several elements of the cosmic rays are shown in fig. 1.5.
The proportions of the major components (with the exception of iron) are relatively constant with energy (see table 1.1).
They are well described by an inverse power law in energy, with differential flux given by . . ..
The spectrum continues up to E~106 GeV with . . .≈1.7; Above this energy the spectrum steepens to . . . ~ 2.0.
An important point to note, however, is that the boron spectrum is steeper than the spectra of its parent oxygen and carbon nuclei.
In fact, all secondary nuclei (i.e. those produced as spallation products of abundant species) have significantly steeper spectra than the primary nuclei.
The secondary to primary ratios decrease as energy increases.
This tells us that the higher energy cosmic rays diffuse out of the galaxy faster.
Cosmic ray composition relative to protons in the 10 - 100 GeV range is shown in table 1.1.
The table shows the fraction of nuclei relative to protons in four different ways.
Fluxes are normally quoted as in column (1): particles per GeV per nucleon.
Academic disciplines periodically undergo reorientation as core themes shift to re-direct research and focus attention on new questions or reexamine traditional problems from new perspectives.
In the humanities and social sciences, a recent line of inquiry has focused on space, prompting scholars of society and culture to talk about a spatial turn within their disciplines.
Even a cursory review of literature reveals the influence of this new direction.
Subject matter, once organized largely as periods and eras, increasingly is ordered under spatial themes, such as region, diaspora, contact zones, and borders or boundaries, among others.
This shift has been accompanied by and reinforced through an equivalent concern with material culture, built and natural environment, and other markers of space and place.
It is not the first time that attention to space and time has reshaped the way we approach social and cultural questions.
A similar turn occurred from 1880 to 1920 when a series of sweeping technological changes created new ways of thinking about time and space.
Distance-collapsing innovations - the telephone, wireless telegraph, radio, cinema, automobiles, and airplanes, among others -challenged traditional understandings of how time and space intersected with the social world.
It suddenly was possible to know events as they occurred, and this experience of simultaneity refashioned people’s sense of distance and direction.
It also meant that individuals were no longer cut off from the flow of time; widely available film and photographic images made the past as accessible as the present, while new developments in science and the world fairs that showcased them made the future seem more definite and real.
New scientific theories, business practices, and cultural forms reinforced the shift:
Einstein’s theory of relativity and Freud’s conception of psychoanalysis shaped consciousness directly; time-management studies, such as Taylorism, dominated manufacturing; and James Joyce and Marcel Proust explored how to link time and space in novels, while the Cubists challenged notions of spatial perspective and form that had long dominated art.
A continuous thread links the first spatial turn with the one we have experienced more recently, but it is likely that this second turn will have a more profound influence on the theory and practice of history and the humanities.
The early twentieth-century conceptions of space and time had less effect on the study of the past than it did on art and literature.
Frederick Jackson Turner’s frontier thesis and its emphasis on the development of the American West and the history it spawned were exceptions, as was the decade-long work of the Annales movement; both schools reflected an intentional focus on questions of space and time.
But the cataclysms of the mid-twentieth century, from world wars and revolutions to mass movements for equality, ultimately spurred historians to search for the roots of momentous events in ideas and politics and technological or social change, causes for which spatial markers were less pronounced.
The considerations for space did not disappear, but they became marked by particularity, an emphasis on place, as scholars began to discern how the story of change differed from one location to another.
The focus on place reflected and reinforced a postmodernist unease with the grand narrative, which created a literature that increasingly became fragmented, with analyses existing at different geographical and temporal scales and few efforts made to link them.
For many humanists, space itself became less geographical and more metaphorical, as scholars found richer meaning in conceptual space - for instance, gendered space, racialized space, or the body as space - than in categories related to the physical environment, the traditional frame of definition for spatial terms.
Today, historians and other humanists are acutely aware of the social and political construction of space and its particular expression as place.
Spaces are not simply the setting for historical action but are a significant product and determinant of change.
They are not passive settings but the medium for the development of culture: “space is not an empty dimension along which social groupings become structured,” sociologist Anthony Giddens notes, “but has to be considered in terms of its involvement in the constitution of systems of interaction” (Giddens 1984, 364).
All spaces contain embedded stories based on what has happened there.
These stories are both individual and collective, and each of them links geography (space) and history (time).
More important, they all reflect the values and cultural codes present in the various political and social arrangements that provide structure to society.
In this sense, then, the meaning of space, especially as place or landscape, is always being constructed through the various contests that occur over power.
There is nothing new in this development - the earliest maps reveal the power arrangements of past societies - but humanities scholarship increasingly reflects what may in fact, by the greatest legacy of postmodernism, the acknowledgment that our understanding of the world itself is socially constructed.
At its core, the spatial turn rejects the universal truths, grand narratives, and structural explanations that dominated the social sciences and the humanities during much of last century.
Above all, it is about the particular and the local, without any supposition that one form of culture is better than another.
Its claim is straightforward: To understand human society and culture, we must understand how it developed in certain circumstances and in certain times and at certain places.
From this knowledge, we can appreciate that the world is not flat but incredibly complicated and diverse.
This view no longer seems new because humanists have embraced it eagerly; now we all recognize the particularity of space, the importance of place.
But for all the uses we make of this insight—and for all its explanatory power—the concepts of space and place employed by historians frequently are metaphorical and not geographical.
Far less often have we grappled with how the physical world has shaped us or how in turn we have shaped perceptions of our material environment.
New spatial technologies, especially geographic information systems (GIS), are now facilitating a (re)discovery of geographical space in history and the other humanities.
At its core, GIS is a powerful software that uses location to integrate and visualize information.
Within a GIS, users can discover relationships that make a complex world more immediately understandable by visually detecting spatial patterns that remain hidden in texts and tables.
Maps have served this function for a long time, but GIS brings impressive computing power to this task.
Its core strength is its ability to integrate, analyze, and make visual a vast array of data from different formats, all by virtue of their shared geography.
This capability has attracted considerable interest from historians, archaeologists, linguists, students of material culture, and others who are interested in place, dense coil of memory, artifact, and experience that exists in a particular space, as well as in the coincidence and movements of people, goods, and ideas that have occurred across time in spaces large and small.
Recent years have witnessed a wide-ranging, if still limited, application of GIS to historical and cultural questions: Did the Dust Bowl of the 1920s and 1930s result from over-farming the land or was it primarily the consequence of larger term environmental changes?
What influence did the rapidly changing cityscape of London have on literature in Elizabethan England?
What is the relationship between rulers and territory in the checkered political landscape of state formation in the nineteenth-century Germany?
How did spatial networks influence the administrative geography of medieval China?
What spatial influences shaped the development of the transcontinental railroad in the United States?
Increasingly, scholars have turned to GIS to provide new perspective on these and other topics that previously have been studied outside of an explicitly spatial framework.
Despite this flurry of interest and activity, most uses to date of GIS in historical and cultural studies have been disparate, application driven, and often tied to the somewhat its more obvious role in census boundary delineation and map making.
While not seeking to minimize the importance of such work, these studies have rarely addressed the broader, more fundamental issues that surround the introduction of a spatial technology such as GIS into the humanities.
There are core reasons why GIS has found early use and ready acceptance in the sciences and social sciences rather than in the more qualitatively based humanities.
The humanities pose far greater epistemological and ontological issues that challenge the technology in a number of ways, from the imprecision and uncertainty of data to concepts of relative space, the use of time as an organizing principle, and the mutually constitutive relationship between time and space.
Essentially, GIS and its related technologies currently allow users to determine a geometry of space.
In the context of the humanities, it will be necessary to move GIS from this more limited quantitative representation of space to facilitate an understanding of place within time and the role that place occupies in humanities disciplines.
In their essence, historians seek to generalize from the particular, not for the purpose of finding universal laws but rather to glean insights about cause and effect from a known outcome.
Here, the humanities differ from much of social science, which attempts to reach a generalization that holds true in any similar circumstance.
This difference is significant and influences the way the two groups of scholars create knowledge.
For many social scientists, the search for trustworthy generalization focuses on the isolation of an independent variable, the cause that has a predictable effect on dependent variables or ones that respond to the stimulus or presence of a catalyst.
They believe it is possible to discover such a variable, given sufficient resources, because the world is not yet lost to them.
Historians must contend with fragmentary evidence and are painfully aware that the past is incomplete and irretrievable.
They also are skeptical of prediction.
The past cannot be changed, even if its interpretation as history is continually in flux, but in it the intersection of patterns and singular events can be discovered.
Not so the future, where continuities and contingencies coexist independently of one another.
Historians view reality as weblike, to use philosopher Michael Oakeshott’s phrase, because they see everything as related in some way to everything else.
Interdependency is the lingua franca of the humanities, and most recently, it has become embodied in practice theory, or in the view of one of its leading proponents, historian William Sewell, “social life may be conceptualized as being composed of countless happenings or encounters in which persons or groups of persons engage in social action.”
In this view, societies and social systems are “continually shaped and reshaped by the creativity and stubbornness of their human creators” (Sewell 2005, 110–111).
Another historian, Ed Ayers, has labeled this concept “deep contingency,” an effort to understand society as a whole with “all structures put into motion and motion put into structures” (Ayers 2010, 7).
The goal of historical scholarship is not to model or replicate the past; a model implies the working out of dependent and independent variables for purposes of prediction, whereas replication suggests the ability to know the past and its cultural forms more completely than most humanists would acknowledge is possible.
Humanists practice an extractive scholarship: they have the capacity for selectivity and shifting perspectives in the pursuit of the fullest possible understanding of heritage and culture.
Traditionally, humanities scholars have used narrative to construct the portrait that furthers this objective.
Narrative encourages the interweaving of evidentiary threads and permits the scholar to qualify, highlight, or subdue any thread or set of them—to use emphasis, nuance, and other literary devices to achieve the complex construction of past worlds.
All of these elements - interdependency, narrative, and nuance, among others - predispose the humanists to look askance at any method or tool that appears to reduce complex events to simple schemes.
Chapter 1 Decision Theory and Human Behavior
People are not logical.
They are psychological.
Anonymous
People often make mistakes in their maths.
This does not mean that we should abandon arithmetic.
Jack Hirshleifer
Decision theory is the analysis of the behavior of an individual facing nonstrategic uncertainty—that is, uncertainty that is due to what we term "Nature” (a stochastic natural event such as a coin flip, seasonal crop loss, personal illness, and the like) or, if other individuals are involved, their behavior is treated as a statistical distribution known to the decision maker.
Decision theory depends on probability theory, which was developed in the seventeenth and eighteenth centuries by such notables as Blaise Pascal, Daniel Bernoulli, and Thomas Bayes.
A rational actor is an individual with consistent preferences (§1.1).
A rational actor need not be selfish.
Indeed, if rationality implied selfishness, the only rational individuals would be sociopaths.
Beliefs, called subjective priors in decision theory, logically stand between choices and payoffs.
Beliefs are primitive data for the rational actor model.
In fact, beliefs are the product of social processes and are shared among individuals.
To stress the importance of beliefs in modeling choice, I often describe the rational actor model as the beliefs, preferences and constraints model, or the BPC model.
The BPC terminology has the added attraction of avoiding the confusing and value-laden term "rational.”
The BPC model requires only preference consistency, which can be defended on basic evolutionary grounds.
While there are eminent critics of preference consistency, their claims are valid in only a few narrow areas.
Because preference consistency does not presuppose unlimited information-processing capacities and perfect knowledge, even bounded rationality (Simon 1982) is consistent with the BPC model.
Because one cannot do behavioral game theory, by which I mean the application of game theory to the experimental study of human behavior, without assuming preference consistency, we must accept this axiom to avoid the analytical weaknesses of the behavioral disciplines that reject the BPC model, including psychology, anthropology, and sociology (see chapter 12).
Behavioral decision theorists have argued that there are important areas in which individuals appear to have inconsistent preferences.
Except when individuals do not know their own preferences, this is a conceptual error based on a misspecification of the decision maker's preference function.
We show in this chapter that, assuming individuals know their preferences, adding information concerning the current state of the individual to the choice space eliminates preference inconsistency.
Moreover, this addition is completely reasonable because preference functions do not make any sense unless we include information about the decision maker's current state.
When we are hungry, scared, sleepy, or sexually deprived, our preference ordering adjusts accordingly.
The idea that we should have a utility function that does not depend on our current wealth, the current time, or our current strategic circumstances is also not plausible.
Traditional decision theory ignores the individual's current state, but this is just an oversight that behavioral decision theory has brought to our attention.
Compelling experiments in behavioral decision theory show that humans violate the principle of expected utility in systematic ways (§1.7).
Again, is must be stressed that this does not imply that humans violate preference consistency over the appropriate choice space but rather that they have incorrect beliefs deriving from what might be termed "folk probability theory” and make systematic performance errors in important cases (Levy 2008).
To understand why this is so, we begin by noting that, with the exception of hyperbolic discounting when time is involved (§1.4), there are no reported failures of the expected utility theorem in nonhumans, and there are some extremely beautiful examples of its satisfaction (Real 1991).
Moreover, territoriality in many species is an indication of loss aversion (Chapter 11).
The difference between humans and other animals is that the latter are tested in real life, or in elaborate simulations of real life, as in Leslie Real's work with bumblebees (1991), where subject bumblebees are released into elaborate spatial models of flowerbeds.
Humans, by contrast, are tested using imperfect analytical models of real-life lotteries.
While it is important to know how humans choose in such situations, there is certainly no guarantee they will make the same choices in the real-life situation and in the situation analytically generated to represent it.
Evolutionary game theory is based on the observation that individuals are more likely to adopt behaviors that appear to be successful for others.
A heuristic that says "adopt risk profiles that appear to have been successful to others” may lead to preference consistency even when individuals are incapable of evaluating analytically presented lotteries in the laboratory.
In addition to the explanatory success of theories based on the BPC model, supporting evidence from contemporary neuroscience suggests that expected utility maximization is not simply an "as if” story.
In fact, the brain's neural circuitry actually makes choices by internally representing the payoffs of various alternatives as neural firing rates and choosing a maximal such rate (Shizgal 1999; Glimcher 2003; Glimcher and Rusti-chini 2004; Glimcher, Dorris, and Bayer 2005).
Neuroscientists increasingly find that an aggregate decision making process in the brain synthesizes all available information into a single unitary value (Parker and Newsome 1998; Schall and Thompson 1999).
Indeed, when animals are tested in a repeated trial setting with variable rewards, dopamine neurons appear to encode the difference between the reward that the animal expected to receive and the reward that the animal actually received on a particular trial (Schultz, Dayan, and Montague 1997; Sutton and Barto 2000), an evaluation mechanism that enhances the environmental sensitivity of the animal's decision making system.
This error prediction mechanism has the drawback of seeking only local optima (Sugrue, Corrado, and Newsome 2005).
Montague and Berns (2002) address this problem, showing that the orbitofrontal cortex and striatum contain a mechanism for more global predictions that include risk assessment and discounting of future rewards.
Their data suggest a decision-making model that is analogous to the famous Black-Scholes options-pricing equation (Black and Scholes 1973).
The existence of an integrated decision-making apparatus in the human brain itself is predicted by evolutionary theory.
The fitness of an organism depends on how effectively it make choices in an uncertain and varying environment.
Effective choice must be a function of the organism's state of knowledge, which consists of the information supplied by the sensory inputs that monitor the organism's internal states and its external environment.
In relatively simple organisms, the choice environment is primitive and is distributed in a decentralized manner over sensory inputs.
But in three separate groups of animals, craniates (vertebrates and related creatures), arthropods (including insects, spiders, and crustaceans), and cephalopods (squid, octopuses, and other mollusks), a central nervous system with a brain (a centrally located decision-making and control apparatus) evolved.
The phylogenetic tree of vertebrates exhibits increasing complexity through time and increasing metabolic and morphological costs of maintaining brain activity.
Thus, the brain evolved because larger and more complex brains, despite their costs, enhanced the fitness of their carriers.
Brains therefore are ineluctably structured to make consistent choices in the face of the various constellations of sensory inputs their bearers commonly experience.
Before the contributions of Bernoulli, Savage, von Neumann, and other experts, no creature on Earth knew how to value a lottery.
The fact that people do not know how to evaluate abstract lotteries does not mean that they lack consistent preferences over the lotteries that they face in their daily lives.
Despite these provisos, experimental evidence on choice under uncertainty is still of great importance because in the modern world we are increasingly called upon to make such "unnatural” choices based on scientific evidence concerning payoffs and their probabilities.
1.1 Beliefs, Preferences, and Constraints
In this section we develop a set of behavioral properties, among which consistency is the most prominent, that together ensure that we can model agents as maximizers of preferences.
A binary relation. . .A on a set A is a subset of A x A.
We usually write the proposition (x,y) . . .A as x . . . A y.
For instance, the arithmetical operator "less than”(') is a binary relation, where (x,y) . . .' is normally written x ' y.
A preference ordering ≥A on A is a binary relation with the following three properties, which must hold for all x,y,z . . . A and any set B:
Complete: x ≥A y or y ≥A x;
Transitive: x ≥A y and y ≥A z imply x≥A z;
Independent of irrelevant alternatives: For x,y . . . B, x≥ By if and only if x≥ A y.
Because of the third property, we need not specify the choice set and can simply write x ≥ y.
We also make the behavioral assumption that given any choice set A, the individual chooses an element x . . .A such that for all y . . . A, x ≥ y.
When x ≥ y, we say "x is weakly preferred to y .”
The first condition is completeness, which implies that any member of A is weakly preferred to itself (for any x in A, x ≥ x).
In general, we say a binary relation . . . is reflexive if, for all x, x. . .x.
Thus, completeness implies reflexivity.
We refer to ≥ as "weak preference” in contrast with "strong preference” '.
We define x ' y to mean "it is false that y ≥ x.”
We say x and y are equivalent if x ≥ y and y ≥ x, and we write x . . . y.
As an exercise, you may use elementary logic to prove that if ≥ satisfies the completeness condition, then andamp;gt; satisfies the following exclusion condition: if x andamp;gt; y, then it is false that y ' x.
The second condition is transitivity, which says that x ≥ y and y ≥ z imply x ≥ z.
It is hard to see how this condition could fail for anything we might like to call a preference ordering.
As a exercise, you may show that x ' y and y ≥ z imply x ' z, and x ≥ y and y ' z imply x ' z.
Similarly, you may use elementary logic to prove that if ≥ satisfies the completeness condition, then . . . is transitive (i.e., satisfies the transitivity condition).
The third condition, independence of irrelevant alternatives (IIA) means that the relative attractiveness of two choices does not depend upon the other choices available to the individual.
For instance, suppose an individual generally prefers meat to fish when eating out, but if the restaurant serves lobster, the individual believes the restaurant serves superior fish, and hence prefers fish to meat, even though he never chooses lobster; thus, IIA fails.
When IIA fails, it can be restored by suitably refining the choice set.
For instance, we can specify two qualities of fish instead of one, in the preceding example.
More generally, if the desirability of an outcome x depends on the set A from which it is chosen, we can form a new choice space . . .*, elements of which are ordered pairs (A,x), where x . . .A. . . *, and restrict choice sets in . . .*to be subsets of . . .* all of whose first elements are equal.
In this new choice space, IIA is trivially satisfied.
When the preference relation ≥is complete, transitive, and independent of irrelevant alternatives, we term it consistent.
If ≥ is a consistent preference relation, then there will always exist a preference function such that the individual behaves as if maximizing this preference function over the set A from which he or she is constrained to choose.
Formally, we say that a preference function u w A → R represents a binary relation ≥ if, for all x,y . . . A, u(x)≥ u(y) if and only if x ≥ y.
We have the following theorem.
Imagine a powerful sovereign who issues commands to his subjects.
They are under a duty to comply with his wishes.
The notion of law as a command lies at the heart of classical legal positivism as espoused by its two great protagonists, Jeremy Bentham and John Austin.
Modern legal positivists adopt a considerably more sophisticated approach to the concept of law, but, like their distinguished predecessors, they deny the relationship proposed by natural law, outlined in the previous chapter, between law and morals.
The claim of natural lawyers that law consists of a series of propositions derived from nature through a process of reasoning is strongly contested by legal positivists.
This chapter describes the essential elements of this important legal theory.
The term ‘positivism’ derives from the Latin positum, which refers to the law as it is laid down or posited.
Broadly speaking, the core of legal positivism is the view that the validity of any law can be traced to an objectively verifiable source.
Put simply, legal positivism, like scientific positivism, rejects the view – held by natural lawyers – that law exists independently from human enactment.
As will become clear in this chapter, the early legal positivism of Bentham and Austin found the origin of law in the command of a sovereign.
H. L. A. Hart looks to a rule of recognition that distinguishes law from other social rules.
Hans Kelsen identifies a basic norm that validates the constitution.
Legal positivists also often claim that there is no necessary connection between law and morals, and that the analysis of legal concepts is worth pursuing, and distinct from (though not hostile to) sociological and historical enquiries and critical evaluation.
The highest common factor among legal positivists is that the law as laid down should be kept separate – for the purpose of study and analysis – from the law as it ought morally to be.
In other words, that a clear distinction must be drawn between ‘ought’ (that which is morally desirable) and ‘is’ (that which actually exists).
But it does not follow from this that a legal positivist is indifferent to moral questions.
Most legal positivists criticize the law and propose means to reform it.
This normally involves moral judgements.
But positivists do share the view that the most effective method of analysing and understanding law involves suspending moral judgement until it is established what it is we are seeking to elucidate.
Nor do positivists necessarily subscribe to the proposition, often ascribed to them, that unjust or iniquitous laws must be obeyed – merely because they are law.
Indeed, both Austin and Bentham acknowledge that disobedience to evil laws is legitimate if it would promote change for the good.
In the words of the foremost modern legal positivist H. L. A. Hart:
[T]he certification of something as legally valid is not conclusive of the question of obedience, ...
[H]owever great the aura of majesty or authority which the official system may have, its demands must in the end be submitted to a moral scrutiny
For Hart, as well as Bentham, this is one of the major virtues of legal positivism.
Law as commands: Bentham and Austin
The prodigious writings of Jeremy Bentham (1748–1832) constitute a major contribution to positivist jurisprudence and the systematic analysis of law and the legal system.
Not only did he seek to expose the shibboleths of his age and construct a comprehensive theory of law, logic, politics, and psychology, founded on the principle of utility, but he essayed for reform of the law on almost every subject.
His critique of the common law and its theoretical underpinnings are especially ferocious.
Moved by the spirit of the Enlightenment, Bentham sought to subject the common law to the cold light of reason.
He attempted to demystify the law, to expose, in his characteristically cutting style, what lay behind its mask.
Appeals to natural law were nothing more than ‘private opinion in disguise’ or ‘the mere opinion of men self-constituted into legislatures’.
The indeterminacy of the common law, he argued, is endemic.
Unwritten law is intrinsically vague and uncertain.
It cannot provide a reliable, public standard which can reasonably be expected to guide behaviour.
The chaos of the common law had to be dealt with systematically.
For Bentham this lay, quite simply, in codification.
Legal codes would significantly diminish the power of judges; their task would consist less of interpreting than administering the law.
It would also remove much of the need for lawyers: the code would be readily comprehensible without the help of legal advisers.
Unlike the Continental system of law that has long adopted Napoleonic codes based on Roman law, codification in the common law world remains a dream.
John Austin (1790–1859) published his major work, The Province of Jurisprudence Determined, in 1832, the year of Bentham’s death.
As a disciple of Bentham’s, Austin’s conception of law is based on the idea of commands or imperatives, though he provides a less elaborate account of what they are.
Both jurists stress the subjection of persons by the sovereign to his power, but Austin’s definition is sometimes thought to extend not very much further than the criminal law, with its emphasis on control over behaviour.
His identification of commands as the hallmark of law leads him to a more restrictive definition of law than is adopted by Bentham who seeks to formulate a single, complete law which sufficiently expresses the legislative will.
But both share a concern to confine the scope of jurisprudential enquiry to accounting for and explaining the principal features of the law.
In the case of Austin, however, his map of ‘law properly so called’ is considerably narrower than Bentham’s, and embraces two categories: the laws of God and human laws.
Human laws (i.e. laws set down by men for men) are further divided into positive laws or laws ‘strictly so called’ (i.e. laws laid down by men as political superiors or in pursuance of legal rights) and laws laid down by men not as political superiors or not in pursuance of legal rights.
Laws ‘improperly so called’ are divided into laws by analogy (e.g. laws of fashion, constitutional, and international law) and by metaphor (e.g. the law of gravity).
Laws by analogy, together with laws set by men not as political superiors or in pursuance of legal right, are merely ‘positive morality’.
It is only positive law that is the proper subject of jurisprudence.
Bentham is best known as a utilitarian (see Chapter 4) and law reformer.
But he insisted on the separation between what he called ‘expositorial’ and ‘censorial’ jurisprudence.
The former describes what is, the latter what ought to be.
Austin was no less categorical in preserving this division, but his analysis is narrower in both its compass and purpose than Bentham’s.
Though both adhere to a utilitarian morality, and adopt broadly similar views on the nature and function of jurisprudence and the serious inadequacies of the common law tradition, there are several important differences in their general approach to the subject.
In particular, Bentham pursues the notion of a single, complete law which adequately expresses the will of the legislature.
He seeks to show how a single law creates a single offence defined by its being the narrowest species of that kind of offence recognized by the law.
Austin, on the other hand, builds his scheme of a legal system on the classification of rights; he is not troubled by a search for a ‘complete’ law.
Also, in his pursuit to provide a plan of a comprehensive body of laws and the elements of the ‘art of legislation’, Bentham expounds a complex ‘logic of the will’.
Austin seeks to construct a science of law rather than engage himself in Bentham’s art of legislation.
And while Bentham sought to devise means by which arbitrary power, especially of judges, might be checked, Austin was less anxious about these matters.
The central feature of Austin’s map of the province of jurisprudence is the notion of law as a command of the sovereign.
Anything that is not a command is not law.
Only general commands count as law.
And only commands emanating from the sovereign are ‘positive laws’.
Austin’s insistence on law as commands requires him to exclude customary, constitutional, and public international law from the field of jurisprudence.
This is because no specific sovereign can be identified as the author of their rules.
Thus, in the case of public international law, sovereign states are notoriously at liberty to disregard its requirements.
For Bentham, however, commands are merely one of four methods by which the sovereign enacts law.
He distinguishes between laws which command or prohibit certain conduct (imperative laws) and those which permit certain conduct (permissive laws).
He argues that all laws are both penal and civil; even in the case of title to property there is a penal element.
Bentham seeks to show that laws which impose no obligations or sanctions (what he calls ‘civil laws’) are not ‘complete laws’, but merely parts of laws.
And, since his principal objective was the creation of a code of law, he argued that the penal and civil branches should be formulated separately.
The relationship between commands and sanctions is no less important for Austin.
Indeed, his very concept of a command includes the probability that a sanction will follow failure to obey the command.
But what is a sanction?
Austin defines it as some harm, pain, or evil that is conditional upon the failure of a person to comply with the wishes of the sovereign.
There must be a realistic probability that it will be inflicted upon anyone who infringes a command.
There need only be the threat of the possibility of a minimal harm, pain, or evil, but unless a sanction is likely to follow, the mere expression of a wish is not a command.
Obligations are therefore defined in terms of sanctions: this is a central tenet of Austin’s imperative theory.
The likelihood of a sanction is always uncertain, but Austin is driven to the rather unsatisfactory position that a sanction consists of ‘the smallest chance of incurring the smallest evil’.
The idea of a sovereign issuing commands pervades the theories of both Bentham and Austin.
It is important to note that both regard the sovereign’s power as constituted by the habit of the people generally obeying his laws.
But while Austin insists on the illimitability and indivisibility of the sovereign, Bentham, alive to the institution of federalism, acknowledges that the supreme legislative power may be both limited and divided by what he calls an express convention.
For Austin, to the four features of a command (wish, sanction, expression of a wish, and generality) is to be added a fifth, namely an identifiable political superior – or sovereign – whose commands are obeyed by political inferiors and who owes obedience to no one.
This insistence on an omnipotent lawgiver distorts those legal systems which impose constitutional restrictions on the legislative competence of the legislature or which divide such power between a central federal legislature and lawmaking bodies of constituent states or provinces (such as in the United States, Canada or Australia).
Bentham, on the other hand, acknowledges that sovereignty may be limited or divided, and accepts (albeit reluctantly) the possibility of judicial review of legislative action.
Austin’s contention that ‘laws properly so called’ be confined to the commands of a sovereign leads him to base his idea of sovereignty on the habit of obedience adopted by members of society.
The sovereign must, moreover, be determinate (i.e. the composition of the sovereign body must be unambiguous), for ‘no indeterminate sovereign can command expressly or tacitly, or can receive obedience or submission’.
And this results in Austin famously refusing to accept as ‘law’ public international law, customary law, and a good deal of constitutional law.
Moreover, by insisting that the sanction is an indispensable ingredient in the definition of law, Austin is driven to defining duty in terms of sanction: if the sovereign expresses a wish and has the power to inflict an evil (or sanction) then a person is under a duty to act in accordance with that wish.
Chapter 10 Graphs
Graphs are discrete structures consisting of vertices and edges that connect these vertices.
There are different kinds of graphs, depending on whether edges have directions, whether multiple edges can connect the same pair of vertices, and whether loops are allowed.
Problems in almost every conceivable discipline can be solved using graph models.
We will give examples to illustrate how graphs are used as models in a variety of areas.
For instance, we will show how graphs are used to represent the competition of different species in an ecological niche, how graphs are used to represent who influences whom in an organization, and how graphs are used to represent the outcomes of round-robin tournaments.
We will describe how graphs can be used to model acquaintanceships between people, collaboration between researchers, telephone calls between telephone numbers, and links between websites.
We will show how graphs can be used to model roadmaps and the assignment of jobs to employees of an organization.
Using graph models, we can determine whether it is possible to walk down all the streets in a city without going down a street twice, and we can find the number of colors needed to color the regions of a map.
Graphs can be used to determine whether a circuit can be implemented on a planar circuit board.
We can distinguish between two chemical compounds with the same molecular formula but different structures using graphs.
We can determine whether two computers are connected by a communications link using graph models of computer networks.
Graphs with weights assigned to their edges can be used to solve problems such as finding the shortest path between two cities in a transportation network.
We can also use graphs to schedule exams and assign channels to television stations.
This chapter will introduce the basic concepts of graph theory and present many different graph models.
To solve the wide variety of problems that can be studied using graphs, we will introduce many different graph algorithms.
We will also study the complexity of these algorithms.
10.1 Graphs and Graph Models
We begin with the definition of a graph.
DEFINITION 1 A graph G = (V, E) consists of V , a nonempty set of vertices (or nodes) and E, a set of edges.
Each edge has either one or two vertices associated with it, called its endpoints.
An edge is said to connect its endpoints.
Remark: The set of vertices V of a graph G may be infinite.
A graph with an infinite vertex set or an infinite number of edges is called an infinite graph, and in comparison, a graph with a finite vertex set and a finite edge set is called a finite graph.
In this book we will usually consider only finite graphs.
Now suppose that a network is made up of data centers and communication links between computers.
We can represent the location of each data center by a point and each communications link by a line segment, as shown in Figure 1.
This computer network can be modeled using a graph in which the vertices of the graph represent the data centers and the edges represent communication links.
In general, we visualize graphs by using points to represent vertices and line segments, possibly curved, to represent edges, where the endpoints of a line segment representing an edge are the points representing the endpoints of the edge.
When we draw a graph, we generally try to draw edges so that they do not cross.
However, this is not necessary because any depiction using points to represent vertices and any form of connection between vertices can be used.
Indeed, there are some graphs that cannot be drawn in the plane without edges crossing (see Section 10.7).
The key point is that the way we draw a graph is arbitrary, as long as the correct connections between vertices are depicted.
Note that each edge of the graph representing this computer network connects two different vertices.
That is, no edge connects a vertex to itself.
Furthermore, no two different edges connect the same pair of vertices.
A graph in which each edge connects two different vertices and where no two edges connect the same pair of vertices is called a simple graph.
Note that in a simple graph, each edge is associated to an unordered pair of vertices, and no other edge is associated to this same edge.
Consequently, when there is an edge of a simple graph associated to {u, v}, we can also say, without possible confusion, that {u, v} is an edge of the graph.
A computer network may contain multiple links between data centers, as shown in Figure 2.
To model such networks we need graphs that have more than one edge connecting the same pair of vertices.
Graphs that may have multiple edges connecting the same vertices are called multigraphs.
When there are m different edges associated to the same unordered pair of vertices {u, v}, we also say that {u, v} is an edge of multiplicity m.
That is, we can think of this set of edges as m different copies of an edge {u, v}.
Sometimes a communications link connects a data center with itself, perhaps a feedback loop for diagnostic purposes.
Such a network is illustrated in Figure 3.
To model this network we need to include edges that connect a vertex to itself.
Such edges are called loops, and sometimes we may even have more than one loop at a vertex.
Graphs that may include loops, and possibly multiple edges connecting the same pair of vertices or a vertex to itself, are sometimes called pseudographs.
So far the graphs we have introduced are undirected graphs.
Their edges are also said to be undirected.
However, to construct a graph model, we may find it necessary to assign directions to the edges of a graph.
For example, in a computer network, some links may operate in only one direction (such links are called single duplex lines).
This may be the case if there is a large amount of traffic sent to some data centers, with little or no traffic going in the opposite direction.
Such a network is shown in Figure 4.
To model such a computer network we use a directed graph.
Each edge of a directed graph is associated to an ordered pair.
The definition of directed graph we give here is more general than the one we used in Chapter 9, where we used directed graphs to represent relations.
DEFINITION 2 A directed graph (or digraph) (V , E) consists of a nonempty set of vertices V and a set of directed edges (or arcs) E.
Each directed edge is associated with an ordered pair of vertices.
The directed edge associated with the ordered pair (u, v) is said to start at u and end at v.
When we depict a directed graph with a line drawing, we use an arrow pointing from u to v to indicate the direction of an edge that starts at u and ends at v. A directed graph may contain loops and it may contain multiple directed edges that start and end at the same vertices.
A directed graph may also contain directed edges that connect vertices u and v in both directions; that is, when a digraph contains an edge from u to v, it may also contain one or more edges from v to u.
Note that we obtain a directed graph when we assign a direction to each edge in an undirected graph.
When a directed graph has no loops and has no multiple directed edges, it is called a simple directed graph.
Because a simple directed graph has at most one edge associated to each ordered pair of vertices (u, v), we call (u, v) an edge if there is an edge associated to it in the graph.
In some computer networks, multiple communication links between two data centers may be present, as illustrated in Figure 5.
Directed graphs that may have multiple directed edges from a vertex to a second (possibly the same) vertex are used to model such networks.
We called such graphs directed multigraphs.
When there are m directed edges, each associated to an ordered pair of vertices (u, v), we say that (u, v) is an edge of multiplicity m.
For some models we may need a graph where some edges are undirected, while others are directed.
A graph with both directed and undirected edges is called a mixed graph.
For example, a mixed graph might be used to model a computer network containing links that operate in both directions and other links that operate only in one direction.
This terminology for the various types of graphs is summarized in Table 1.
We will sometimes use the term graph as a general term to describe graphs with directed or undirected edges (or both), with or without loops, and with or without multiple edges.
At other times, when the context is clear, we will use the term graph to refer only to undirected graphs.
Because of the relatively modern interest in graph theory, and because it has applications to a wide variety of disciplines, many different terminologies of graph theory have been introduced.
The reader should determine how such terms are being used whenever they are encountered.
The terminology used by mathematicians to describe graphs has been increasingly standardized, but the terminology used to discuss graphs when they are used in other disciplines is still quite varied.
Although the terminology used to describe graphs may vary, three key questions can help us understand the structure of a graph:
Are the edges of the graph undirected or directed (or both)?
If the graph is undirected, are multiple edges present that connect the same pair of vertices?
If the graph is directed, are multiple directed edges present?
Are loops present?
Answering such questions helps us understand graphs.
It is less important to remember the particular terminology used.
10.1.1 Graph Models
Graphs are used in a wide variety of models.
We began this section by describing how to construct graph models of communications networks linking data centers.
We will complete this section by describing some diverse graph models for some interesting applications.
We will return to many of these applications later in this chapter and in Chapter 11.
We will introduce additional graph models in subsequent sections of this and later chapters.
Also, recall that directed graph models for some applications were introduced in Chapter 9.
When we build a graph model, we need to make sure that we have correctly answered the three key questions we posed about the structure of a graph.
SOCIAL NETWORKS Graphs are extensively used to model social structures based on different kinds of relationships between people or groups of people.
These social structures, and the graphs that represent them, are known as social networks.
In these graph models, individuals or organizations are represented by vertices; relationships between individuals or organizations are represented by edges.
The study of social networks is an extremely active multidisciplinary area, and many different types of relationships between people have been studied using them.
We will introduce some of the most commonly studied social networks here.
EXAMPLE 1 Acquaintanceship and Friendship Graphs We can use a simple graph to represent whether two people know each other, that is, whether they are acquainted, or whether they are friends (either in the real world in the virtual world via a social networking site such as Facebook).
Each person in a particular group of people is represented by a vertex.
An undirected edge is used to connect two people when these people know each other, when we are concerned only with acquaintanceship, or whether they are friends.
No multiple edges and usually no loops are used.
(If we want to include the notion of self-knowledge, we would include loops.)
A small acquaintanceship graph is shown in Figure 6.
The acquaintanceship graph of all people in the world has more than six billion vertices and probably more than one trillion edges!
We will discuss this graph further in Section 10.4.
Chapter 1 Definitions and Nomenclature
This chapter presents some basic radar definitions and establishes much of the nomenclature used throughout this text.
The word radar is an abbreviation for radio detection and ranging.
In most cases, radar systems use modulated waveforms and directive antennas to transmit electromagnetic energy into a specific volume in space to search for targets.
Objects (targets) within a search volume will reflect portions of the incident energy (radar returns or echoes) in the direction of the radar.
These echoes are then processed by the radar receiver to extract target information such as range, velocity, angular position, and other target identifying characteristics.
1.1.Radar Systems Classifications and Bands
Radars can be classified as ground-based, airborne, spaceborne, or ship-based radar systems.
They can also be classified into numerous categories based on the specific radar characteristics, such as the frequency band, antenna type, and waveforms utilized.
Radar systems using continuous waveforms, modulated or otherwise, are classified as Continuous Wave (CW) radars.
Alternatively, radar systems using time-limited pulsed waveforms are classified as Pulsed Radars.
Another radar systems classification is concerned with the mission and/or the functionality of the specific radar.
This includes: weather, acquisition and search, tracking, track-while-scan, fire control, early warning, over-the-horizon, terrain following, and terrain avoidance radars.
Phased array radars utilize phased array antennas, and are often called multifunction (multimode) radars.
A phased array is a composite antenna formed from two or more basic radiators.
Array antennas synthesize narrow directive beams that may be steered, mechanically or electronically.
Electronic steering is achieved by controlling the phase of the electric current feeding the array elements, and thus the name phased arrays is adopted.
Historically, radars were first developed as military tools.
It is for this primary reason the most common radar systems classification is the letter or band designation originally used by the military during and after World War II.
This letter or band designation has also been adopted as an IEEE (Institute of Electrical and Electronics Engineers) standard.
In recent years, NATO (North Atlantic Treaty Organization) has adopted a new band designation with easier abecedarian letters.
Figure 1.1 shows the spectrum associated with these two letter or band radar classifications, while Table 1.1 presents the same information in a structured format.
Figure 1.1. Radar systems band or letter classification.
Table 1.1. Radar systems band or letter classification.
High Frequency (HF) and Very High Frequency (VHF) Radars (A- and B-Bands):
These radar bands below 300MHz represent the frontier of radio technology at the time during the World War II.
However, in the modern radar era, these frequencies bands are used for early warning radars.
These radars utilize the electromagnetic waves’ reflection off the ionosphere to detect targets beyond the horizon, and so they are called Over-the-Horizon Radars (OTHR).
Some examples include the United States (U.S.)
Navy Relocatable over-the-horizon Radar (ROTHR) shown in Fig. 1.2, and the Russian Woodpecker radar shown in Fig. 1.3.
By using these low HF and VHF frequency bands, one can use high-power transmitters.
At these frequencies, the electromagnetic wave atmospheric attenuation is small and can be overcome by using high-power transmitters.
Radar angular measurement accuracies are limited in these bands because lower frequencies require antennas with significant physical size, thus limiting the radar’s angle accuracy and angle resolution.
Other communication and broadcasting services typically use these frequency bands.
Therefore, the available bandwidth for military radar systems is limited and highly contested throughout the world.
Low-frequency systems can be used for Foliage Penetration (FoPen) applications, as well as in Ground Penetrating (GPen) applications.
Figure 1.2. U. S. Navy over-the-horizon Radar.
Figure 1.3. Russian Woodpecker OTHR radar.
Ultra High Frequency (UHF) Radars (C-Band): UHF bands are used for very long range Early Warning Radars (EWR).
Some examples include the Ballistic Missile Early Warning System (BMEWS) search-and-track monopulse radar that operates at 245MHz (see Fig. 1.4), the Perimeter and Acquisition Radar (PAR), which is a very long range multifunction phased array radar; and the early warning PAVE PAWS multifunction UHF phased array radar.
This frequency band is also used for the detection and tracking of satellites and ballistic missiles over a long range.
In recent years, ultra wideband (UWB) radar applications use all frequencies in the A- to C-Bands.
UWB radars can be used in GPen applications as well as in see-through- the-wall applications.
Figure 1.4. Fylingdales BMEWS, United Kingdom.
L-Band Radars (D-Band): Radars in the L-band are primarily ground-based and ship- based systems that are used in long range military and air traffic control search operations for up to 250 (~500Km) nautical miles.
Therefore, due to earth curvature their maximum achievable range is limited when detecting low-altitude targets which can disappear very quickly below the horizon.
The Air Traffic Management (ATM) long-range surveillance radars like the Air Route Surveillance Radar (ARSR), work in this frequency band.
These radar systems are relatively large and demand sizable footprints.
Historically, the designator L-Band was adopted since the “L” represent with large antenna or long range radars.
S-Band Radars (E- and F-Bands): Most ground- and ship-based medium range radars operate in the S-band.
For example, the Airport Surveillance Radar (ASR) used for air traffic control, and the ship-based U.S. Navy AEGIS (Fig. 1.5) multifunction phased array are S-band radars, and the Airborne Warning and Control System (AWACS) shown in Fig. 1.6.
The atmospheric attenuation in this band is higher than in the D-Band, and they are also more susceptible to weather conditions.
Radar in this band usually need considerably high transmitting power as compared to the lower-frequency radars in order to achieve maximum detection range.
Even with the considerable weather susceptibility, the National Weather Service Next Generation Doppler Weather Radar (NEXRAD) uses an S-band radar, because it can see beyond a severe storm.
Special Airport Surveillance Radars (ASR) used at some civilian airports are also in this band where they can detect aircrafts for up to 60 nautical miles.
The designator S-Band (contrary to L-Band) was adopted since the “S” represents the smaller antennas or shorter range radars.
Figure 1.5. U. S. Navy AEGIS.
Figure 1.6. U. S. Air Force AWACS.
C-Band Radar (G-Band): Many of the mobile military battlefield surveillance, missile- control and ground surveillance radar systems operate in this band.
Most weather radar systems are also C-band radars.
Medium range search and fire control military radars and metric instrumentation radars are C-band systems.
In this band, the size of the antenna allows for achieving excellent angular accuracies and resolution.
Performance of systems operating in this band suffer severely from bad weather conditions and to counter that, they often employ antenna feeds with circular polarization.
X- and Ku-Band Radars (I- and J-Bands): In the X-band frequency range (8 to 12GHz) the relationship between the wave length and size of the antenna is considerably better than in lower-frequency bands.
Radar systems that require fine target detection capabilities and yet cannot tolerate the atmospheric attenuation of higher-frequency bands are typically X-Band.
The X- and Ku-bands are relatively popular radar frequency bands for military applications like airborne radars, since the small antenna size provides good performance.
Missile guidance systems use the Ku-Band (I- and J-Bands) because of the convenient antenna size where weight is a limiting requirement.
Space borne or airborne imaging radars used in Synthetic Aperture Radar (SAR) for military electronic intelligence and civil geographic mapping typically use these frequency bands.
Finally, these frequency bands are also widely used in maritime civil and military navigation radars.
K- and Ka- Band Radars (J- and K-Bands): These high-frequency bands suffer severe weather and atmospheric attenuation.
Therefore, radars utilizing these frequency bands are limited to short range applications, such as police traffic radars, short range terrain avoidance, and terrain following radars.
Alternatively, the achievable angular accuracies and range resolution are superior to other bands.
In ATM applications these radars are often called Surface Movement Radar (SMR) or Airport Surface Detection Equipment (ASDE) radars.
Millimeter Wave (MMW) Radars (V- and W-Bands): Radars operating in this frequency band also suffer from severe high atmospheric attenuation.
Radar applications are limited to very short range of up to a tens of meters.
In the W-Band maximum attenuation occurs at about 75GHz and at about 96GHz.
Both of these frequencies are used in practice primarily in automotive industry where very small radars (~75-76GHz) are used for parking assistants, blind spot and brake assists.
Some radar systems operating at 96 to 98GHz are used as laboratory experimental or prototype systems.
1.2. Pulsed and Continuous Wave (CW) Radars
When the type of waveform is used as a classifier of radar systems, there are two types of radars; pulsed and Continuous Wave (CW) radar systems.
Continuous wave radars are those that continuously emit electromagnetic energy, and use separate transmit and receive antennas.
Unmodulated CW radars can accurately measure target radial velocity (Doppler shift) and angular position.
Continuous wave waveforms can be viewed as pure sinewaves of the form cos2πf0t.
Spectra of the radar echo from stationary targets and clutter will be concentrated around f0.
The center frequency for the echoes of a moving target will be shifted by fd, the Doppler frequency.
Thus, by measuring this frequency difference, CW radars can very accurately extract target radial velocity.
Because of the continuous nature of CW emission, range measurement is not possible without some modifications to the radar operations and waveforms.
Simply put, target range information cannot be extracted without utilizing some form of modulation.
The primary use of CW radars is in target velocity search and track, and in missile guidance operations.
Pulsed radars use a train of pulsed waveforms (mainly with modulation).
In this category, radar systems can be classified on the basis of the Pulse Repetition Frequency (PRF), as low PRF, medium PRF, and high PRF radars.
Low PRF radars are primarily used for ranging where target velocity (Doppler shift) is not of interest.
High PRF radars are mainly used to measure target velocity.
Continuous wave as well as pulsed radars can measure both target range and radial velocity by utilizing different modulation schemes.
The design, operation, and analysis of CW and pulsed radar systems are found in subsequent chapters of this book.
1.3. Range
Figure 1.7 shows a simplified pulsed radar block diagram.
The time control box generates the synchronization timing signals required throughout the system.
A modulated signal is generated and sent to the antenna by the modulator/transmitter block.
Switching the antenna between the transmitting and receiving modes is controlled by the duplexer.
The duplexer allows one antenna to be used to both transmit and receive.
During transmission it directs the radar electromagnetic energy toward the antenna.
Alternatively, on reception, it directs the received radar echoes to the receiver.
The receiver amplifies the radar returns and prepares them for signal processing.
Extraction of target information is performed by the signal processor block.
The target’s range, R, is computed by measuring the time delay, Δt; it takes a pulse to travel the two-way path between the radar and the target.
Since electromagnetic waves travel at the speed of light, c = 3×108 m/s, then . . . where R is in meters and Δt is in seconds.
The factor of 1/2 is used to account for the two-way time delay.
In general, a pulsed radar transmits and receives a train of pulses, as illustrated by Fig. 1.8.
The Inter Pulse Period (IPP) is T, and the pulse width is τ.
The IPP is often referred to as the Pulse Repetition Interval (PRI).
The inverse of the PRI is the PRF, which is denoted by ft, . . .
During each PRI the radar radiates energy only for . . . seconds and listens for target returns for the rest of the PRI.
The radar transmitting duty cycle (factor) dt is defined as the ratio . . ..
2 Mathematical Modeling of Control Systems
2.1 INTRODUCTION
In studying control systems the reader must be able to model dynamic systems in mathematical terms and analyze their dynamic characteristics.
A mathematical model of a dynamic system is defined as a set of equations that represents the dynamics of the system accurately, or at least fairly well.
Note that a mathematical model is not unique to a given system.
A system may be represented in many different ways and, therefore, may have many mathematical models, depending on one’s perspective.
The dynamics of many systems, whether they are mechanical, electrical, thermal, economic, biological, and so on, may be described in terms of differential equations.
Such differential equations may be obtained by using physical laws governing a particular system--for example, Newton’s laws for mechanical systems and Kirchhoff’s laws for electrical systems.
We must always keep in mind that deriving reasonable mathematical models is the most important part of the entire analysis of control systems.
Throughout this book we assume that the principle of causality applies to the systems considered.
This means that the current output of the system (the output at time t=0) depends on the past input (the input for t'0) but does not depend on the future input (the input for t'0).
Mathematical Models.
Mathematical models may assume many different forms.
Depending on the particular system and the particular circumstances, one mathematical model may be better suited than other models.
For example, in optimal control problems, it is advantageous to use state-space representations.
On the other hand, for the transient-response or frequency-response analysis of single-input, single-output, linear, time-invariant systems, the transfer-function representation may be more convenient than any other.
Once a mathematical model of a system is obtained, various analytical and computer tools can be used for analysis and synthesis purposes.
Simplicity Versus Accuracy.
In obtaining a mathematical model, we must make a compromise between the simplicity of the model and the accuracy of the results of the analysis.
In deriving a reasonably simplified mathematical model, we frequently find it necessary to ignore certain inherent physical properties of the system.
In particular, if a linear lumped-parameter mathematical model (that is, one employing ordinary differential equations) is desired, it is always necessary to ignore certain nonlinearities and distributed parameters that may be present in the physical system.
If the effects that these ignored properties have on the response are small, good agreement will be obtained between the results of the analysis of a mathematical model and the results of the experimental study of the physical system.
In general, in solving a new problem, it is desirable to build a simplified model so that we can get a general feeling for the solution.
A more complete mathematical model may then be built and used for a more accurate analysis.
We must be well aware that a linear lumped-parameter model, which may be valid in low-frequency operations, may not be valid at sufficiently high frequencies, since the neglected property of distributed parameters may become an important factor in the dynamic behavior of the system.
For example, the mass of a spring may be neglected in low-frequency operations, but it becomes an important property of the system at high frequencies.
(For the case where a mathematical model involves considerable errors, robust control theory may be applied.
Robust control theory is presented in Chapter 10.)
Linear Systems.
A system is called linear if the principle of superposition applies.
The principle of superposition states that the response produced by the simultaneous application of two different forcing functions is the sum of the two individual responses.
Hence, for the linear system, the response to several inputs can be calculated by treating one input at a time and adding the results.
It is this principle that allows one to build up complicated solutions to the linear differential equation from simple solutions.
In an experimental investigation of a dynamic system, if cause and effect are proportional, thus implying that the principle of superposition holds, then the system can be considered linear.
Linear Time-Invariant Systems and Linear Time-Varying Systems.
A differential equation is linear if the coefficients are constants or functions only of the independent variable.
Dynamic systems that are composed of linear time-invariant lumped-parameter components may be described by linear time-invariant differential equations--that is, constant-coefficient differential equations.
Such systems are called linear time-invariant (or linear constant-coefficient) systems.
Systems that are represented by differential equations whose coefficients are functions of time are called linear time-varying systems.
An example of a time-varying control system is a spacecraft control system.
(The mass of a spacecraft changes due to fuel consumption.)
In control theory, functions called transfer functions are commonly used to characterize the input-output relationships of components or systems that can be described by linear, time-invariant, differential equations.
We begin by defining the transfer function and follow with a derivation of the transfer function of a differential equation system.
Then we discuss the impulse-response function.
2.2.1 Transfer Function.
The transfer function of a linear, time-invariant, differential equation system is defined as the ratio of the Laplace transform of the output (response function) to the Laplace transform of the input (driving function) under the assumption that all initial conditions are zero.
Consider the linear time-invariant system defined by the following differential equation: . . . where y is the output of the system and x is the input.
The transfer function of this system is the ratio of the Laplace transformed output to the Laplace transformed input when all initial conditions are zero, or . . .
By using the concept of transfer function, it is possible to represent system dynamics by algebraic equations in s.
If the highest power of s in the denominator of the transfer function is equal to n, the system is called an nth-order system.
2.2.2 Comments on Transfer Function.
The applicability of the concept of the transfer function is limited to linear, time-invariant, differential equation systems.
The transfer function approach, however, is extensively used in the analysis and design of such systems.
In what follows, we shall list important comments concerning the transfer function.
(Note that a system referred to in the list is one described by a linear, time-invariant, differential equation.)
1.The transfer function of a system is a mathematical model in that it is an operational method of expressing the differential equation that relates the output variable to the input variable.
2. The transfer function is a property of a system itself, independent of the magnitude and nature of the input or driving function.
3. The transfer function includes the units necessary to relate the input to the output; however, it does not provide any information concerning the physical structure of the system.
(The transfer functions of many physically different systems can be identical.)
4. If the transfer function of a system is known, the output or response can be studied for various forms of inputs with a view toward understanding the nature of the system.
5. If the transfer function of a system is unknown, it may be established experimentally by introducing known inputs and studying the output of the system.
Once established, a transfer function gives a full description of the dynamic characteristics of the system, as distinct from its physical description.
2.2.3 Convolution Integral.
For a linear, time-invariant system the transfer function G(s) is . . . where X(s) is the Laplace transform of the input to the system and Y(s) is the Laplace transform of the output of the system, where we assume that all initial conditions involved are zero.
It follows that the output Y(s) can be written as the product of G(s) and X(s), or . . .
Note that multiplication in the complex domain is equivalent to convolution in the time domain (see Appendix A), so the inverse Laplace transform of Equation (2-1) is given by the following convolution integral: . . . where both g(t) and x(t) are 0 for t'0.
2.2.4 Impulse-Response Function.
Consider the output (response) of a linear time-invariant system to a unit-impulse input when the initial conditions are zero.
Since the Laplace transform of the unit-impulse function is unity, the Laplace transform of the output of the system is . . .
The inverse Laplace transform of the output given by Equation (2.2) gives the impulse response of the system.
The inverse Laplace transform of G(s), or . . . is called the impulse-response function.
This function g(t) is also called the weighting function of the system.
The impulse-response function g(t) is thus the response of a linear time-invariant system to a unit-impulse input when the initial conditions are zero.
The Laplace transform of this function gives the transfer function.
Therefore, the transfer function and impulse-response function of a linear, time-invariant system contain the same information about the system dynamics.
It is hence possible to obtain complete information about the dynamic characteristics of the system by exciting it with an impulse input and measuring the response.
(In practice, a pulse input with a very short duration compared with the significant time constants of the system can be considered an impulse.)
2.3 AUTOMATIC CONTROL SYSTEMS
A control system may consist of a number of components.
To show the functions performed by each component, in control engineering, we commonly use a diagram called the block diagram.
This section first explains what a block diagram is.
Next, it discusses introductory aspects of automatic control systems, including various control actions.
Then, it presents a method for obtaining block diagrams for physical systems, and, finally, discusses techniques to simplify such diagrams.
2.3.1 Block Diagrams.
A block diagram of a system is a pictorial representation of the functions performed by each component and of the flow of signals.
Such a diagram depicts the interrelationships that exist among the various components.
Differing from a purely abstract mathematical representation, a block diagram has the advantage of indicating more realistically the signal flows of the actual system.
In a block diagram all system variables are linked to each other through functional blocks.
The functional block or simply block is a symbol for the mathematical operation on the input signal to the block that produces the output.
The transfer functions of the components are usually entered in the corresponding blocks, which are connected by arrows to indicate the direction of the flow of signals.
Note that the signal can pass only in the direction of the arrows.
Thus a block diagram of a control system explicitly shows a unilateral property.
Figure 2-1 shows an element of the block diagram.
The arrowhead pointing toward the block indicates the input, and the arrowhead leading away from the block represents the output.
Such arrows are referred to as signals.
Note that the dimension of the output signal from the block is the dimension of the input signal multiplied by the dimension of the transfer function in the block.
The advantages of the block diagram representation of a system are that it is easy to form the overall block diagram for the entire system by merely connecting the blocks of the components according to the signal flow and that it is possible to evaluate the contribution of each component to the overall performance of the system.
In general, the functional operation of the system can be visualized more readily by examining the block diagram than by examining the physical system itself.
A block diagram contains information concerning dynamic behavior, but it does not include any information on the physical construction of the system.
Consequently, many dissimilar and unrelated systems can be represented by the same block diagram.
It should be noted that in a block diagram the main source of energy is not explicitly shown and that the block diagram of a given system is not unique.
A number of different block diagrams can be drawn for a system, depending on the point of view of the analysis.
CHAPTER 3 AUTOIMMUNITY AND AUTOIMMUNE DISEASES
One of the central features of the immune system is the capacity to mount an inflammatory response to nonself while avoiding harm to self tissues.
While recognition of self plays an important role in shaping the repertoires of immune receptors on both T and B cells, and in the clearance of apoptotic debris from tissues throughout the body, the development of potentially harmful immune responses to self-antigens is, in general, precluded.
The essential feature of an autoimmune disease is that tissue injury is caused by the immunologic reaction of the organism against its own tissues.
Autoimmunity, on the other hand, refers merely to the presence of antibodies or T lymphocytes that react with self-antigens and does not necessarily imply that the self-reactivity has pathogenic consequences.
Autoimmunity is present in all individuals; however, autoimmune disease represents the end result of the breakdown of one or more of the basic mechanisms regulating immune tolerance.
Autoimmunity is seen in normal individuals and in higher frequency in normal older people.
Polyreactive autoantibodies that recognize many host antigens are present throughout life.
Expression of these antibodies may be increased following some inciting events.
These are usually of the IgM heavy chain isotype and are encoded by nonmutated germline immunoglobulin variable region genes.
When autoimmunity is induced by an inciting event, such as infection or tissue damage from trauma or ischemia, the autoreactivity is in general self-limited.
Such autoimmunity may, however, be persistent, and then may or may not result in ensuing pathology.
Even in the presence of organ pathology, it may be difficult to determine whether the damage is mediated by autoreactivity.
Following an inciting event, the development of self-reactivity may be the consequence of an ongoing pathologic process, and be non-pathogenic, or may contribute to tissue inflammation and damage.
MECHANISMS OF AUTOIMMUNITY
Since Ehrlich first postulated the existence of mechanisms to prevent the generation of self-reactivity in 1900, ideas concerning the nature of this inhibition have developed in parallel with a progressive increase in understanding of the immune system.
Burnet's clonal selection theory included the idea that interaction of lymphoid cells with their specific antigens during fetal or early postnatal life would lead to elimination of such "forbidden clones."
This idea became untenable, however, when it was shown that autoimmune diseases could be induced in experimental animals by simple immunization procedures, that autoantigen-binding cells could be demonstrated easily in the circulation of normal individuals, and that self-limited autoimmune phenomena frequently developed following tissue damage from infection or trauma.
These observations indicated that clones of cells capable of responding to autoantigens were present in the repertoire of antigen-reactive cells in normal adults and suggested that mechanisms in addition to clonal deletion were responsible for preventing their activation.
Currently, three general processes are thought to be involved in the maintenance of selective unresponsiveness to autoantigens (Table 3-1): (1) sequestration of self-antigens, rendering them inaccessible to the immune system; (2) specific unresponsiveness (tolerance or anergy) of relevant T or B cells; and (3) limitation of potential reactivity by regulatory mechanisms.
Derangements of these normal processes may predispose to the development of autoimmunity (Table 3-2).
In general, these abnormal responses require an exogenous trigger such as bacterial or viral infection or cigarette smoking and require the presence of endogenous abnormalities in the cells of the immune system.
Microbial superantigens, such as staphylococcal protein A and staphylococcal enterotoxins, are substances that can stimulate a broad range of T and B cells based upon specific interactions with selected families of immune receptors, irrespective of their antigen specificity.
If autoantigen-reactive T and/or B cells express these receptors, autoimmunity might develop.
Alternatively, molecular mimicry or cross-reactivity between a microbial product and a self-antigen might lead to activation of autoreactive lymphocytes.
One of the best examples of autoreactivity and autoimmune disease resulting from molecular mimicry is rheumatic fever, in which antibodies to the M protein of streptococci cross-react with myosin, laminin, and other matrix proteins as well as neuronal antigens.
Deposition of these autoantibodies in the heart initiates an inflammatory response, whereas penetration of these antibodies into the brain can result in Sydenham's chorea.
Molecular mimicry between microbial proteins and host tissues has been reported in type 1 diabetes mellitus, rheumatoid arthritis, and multiple sclerosis.
It is presumed that infectious agents may be able to overcome self-tolerance because they possess molecules, such as bacterial endotoxin, RNA, or DNA, that have adjuvant-like effects on the immune system that increase the immunogenicity of the microbial antigens.
The adjuvants activate dendritic cells through pattern recognition receptors and stimulate the activation of previously quiescent lymphocytes that recognize both microbial and self-antigen.
Endogenous derangements of the immune system may also contribute to the loss of immunologic tolerance to self-antigens and the development of autoimmunity (Table 3-2).
Some autoantigens reside in immunologically privileged sites, such as the brain or the anterior chamber of the eye.
These sites are characterized by the inability of engrafted tissue to elicit immune responses.
Immunologic privilege results from a number of events, including the limited entry of proteins from those sites into lymphatics, the local production of immunosuppressive cytokines such as transforming growth factor . . ., and the local expression of molecules such as Fas ligand that can induce apoptosis of activated T cells.
Lymphoid cells remain in a state of immunologic ignorance (neither activated nor anergized) to proteins expressed uniquely in immunologically privileged sites.
If the privileged site is damaged by trauma or inflammation, or if T cells are activated elsewhere, proteins expressed at this site can become the targets of immunologic assault.
Such an event may occur in multiple sclerosis and sympathetic ophthalmia, in which antigens uniquely expressed in the brain and eye, respectively, become the target of activated T cells.
Alterations in antigen presentation may also contribute to autoimmunity.
Peptide determinants (epitopes) of a self-antigen that are not routinely presented to lymphocytes may be recognized as a result of altered proteolytic processing of the molecule and the ensuing presentation of novel peptides (cryptic epitopes).
When B cells rather than dendritic cells present self-antigen, they may also present cryptic epitopes that can activate autoreactive T cells.
These cryptic epitopes will not have previously been available to effect the silencing of autoreactive lymphocytes.
Furthermore, once there is immunologic recognition of one protein component of a multimolecular complex, reactivity may be induced to other components of the complex following internalization and presentation of all molecules within the complex (epitope spreading).
Finally, inflammation, drug exposure, or normal senescence may cause a primary chemical alteration in proteins, resulting in the generation of immune responses that cross-react with normal self-proteins.
For example, the induction and/or release of protein arginine deaminase enzymes results in the conversion of arginine residues to citrullines in a variety of proteins, thereby altering their capacity to induce immune responses.
Production of anticitrullinated protein antibodies has been observed in rheumatoid arthritis, chronic lung disease, as well as normal smokers and may contribute to organ pathology.
Alterations in the availability and presentation of autoantigens may be important components of immunoreactivity in certain models of organ-specific autoimmune diseases.
In addition, these factors may be relevant in understanding the pathogenesis of various drug-induced autoimmune conditions.
However, the diversity of autoreactivity manifest in non-organ-specific systemic autoimmune diseases suggests that these conditions might result from a more general activation of the immune system rather than from an alteration in individual self-antigens.
Many autoimmune diseases are characterized by the presence of antibodies that react with apoptotic material.
Defects in the clearance of apoptotic material have been shown to elicit autoimmunity and autoimmune disease in a number of animal models.
Moreover, defects in the clearance of apoptotic material have been found in subjects with systemic lupus erythematosus (SLE).
Apoptotic debris not quickly cleared by the immune system can function as endogenous ligands for a number of pattern recognition receptors on dendritic cells.
Under such circumstances, there is activation of dendritic cells, and an immune response to apoptotic debris can develop.
In addition, the presence of extracellular apoptotic material within germinal centers of secondary lymphoid organs may facilitate the direct activation of autoimmune B cell clones or function to select autoimmune B cell clones during immune responses.
A number of experimental models have suggested that intense stimulation of T lymphocytes can produce nonspecific signals that bypass the need for antigen-specific helper T cells and lead to polyclonal B cell activation with the formation of multiple autoantibodies.
For example, antinuclear, antierythrocyte, and antilymphocyte antibodies are produced during the chronic graft-versus-host reaction.
In addition, true autoimmune diseases, including autoimmune hemolytic anemia and immune complex–mediated glomerulonephritis, can also be induced in this manner.
While it is clear that such diffuse activation of helper T cell activity can cause autoimmunity, nonspecific stimulation of B lymphocytes can also lead to the production of autoantibodies.
Thus, the administration of polyclonal B cell activators, such as bacterial endotoxin, to normal mice leads to the production of a number of autoantibodies, including those directed to DNA and IgG (rheumatoid factor).
Moreover, excess BAFF can also cause T cell–independent B cell activation and heavy chain class switching and the development of autoimmunity.
SLE, for example, can be induced in mice through exuberant dendritic cell activation, a redundancy of TLR7 on the y chromosome (BXSByaa mice) or through exposure to CpG, a ligand for TLR 9.
The ensuing induction of inflammatory mediators can cause a switch from production of nonpathogenic IgM autoantibodies to pathogenic IgG autoantibodies in the absence of antigen-specific T cell help.
Aberrant selection of the B or T cell repertoire at the time of antigen receptor expression can also predispose to autoimmunity.
For example, B cell immunodeficiency caused by an absence of the B cell receptor–associated kinase, Bruton's tyrosine kinase, leads to X-linked agammaglobulinemia.
This syndrome is characterized by reduced B cell activation, but also by diminished negative selection of autoreactive B cells probably caused by high levels of BAFF, resulting in increased autoreactivity within a diminished B cell repertoire.
Likewise, negative selection of autoreactive T cells in the thymus requires expression of the autoimmune regulator (AIRE) gene that enables the expression of tissue-specific proteins in thymic medullary epithelial cells.
Peptides from these proteins are expressed in the context of major histocompatibility complex (MHC) molecules and mediate the elimination of autoreactive T cells.
The absence of AIRE gene expression leads to a failure of negative selection of autoreactive cells, autoantibody production, and severe inflammatory destruction of multiple organs.
Individuals deficient in AIRE gene expression develop autoimmune polyendocrinopathy-candidiasis-ectodermal dystrophy (APECED).
Primary alterations in the activity of T and/or B cells, cytokine imbalances, or defective immunoregulatory circuits may also contribute to the emergence of autoimmunity.
Diminished production of tumor necrosis factor (TNF) and interleukin (IL) 10 has been reported to be associated with the development of autoimmunity.
Overproduction of type 1 interferon has also been associated with autoimmunity.
Overexpression of co-stimulatory molecules on T cells similarly can lead to autoantibody production.
Autoimmunity may also result from an abnormality of immunoregulatory mechanisms.
Observations made in both human autoimmune disease and animal models suggest that defects in the generation and expression of regulatory T cell activity may allow for the production of autoimmunity.
It has recently been appreciated that the IPEX (immunodysregulation, polyendocrinopathy, enteropathy X-linked) syndrome results from the failure to express the FOXP3 gene, which encodes a molecule critical in the differentiation of regulatory T cells.
Administration of normal regulatory T cells or factors derived from them can prevent the development of autoimmune disease in rodent models of autoimmunity.
Abnormalities in the function of regulatory T cells have been noted in a number of human autoimmune diseases, although it remains uncertain whether these are causative or are secondary abnormalities owing to inflammation.
Finally, recent data indicate that B cells may also exert regulatory function, largely through the production of the cytokine IL-10.
Deficiency of IL- 10-producing regulatory B cells can prolong the course of an animal model of multiple sclerosis.
It should be apparent that no single mechanism can explain all the varied manifestations of autoimmunity.
Furthermore, genetic evaluation has shown that a number of abnormalities often need to converge to induce an autoimmune disease.
